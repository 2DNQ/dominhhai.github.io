---
title: "[ML] Tối ưu hàm lỗi với Gradient Descent"
slug: ml-gd
date: 2017-12-22T08:45:04+09:00
categories:
- Học Máy
- ML
tags:
- Học Máy
keywords:
- Học Máy
- Machine Learning
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
draft: true
---
Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là **Gradient Descent** thường được sử dụng.
<!--more-->

Chính vì vậy tôi viết bài này để hiểu rõ hơn về phương pháp tối ưu bằng giải thuật **Gradient Descent** cũng như các biến thể và tối ưu cho giải thuật này. Để cho thuận tiện từ giờ trở đi tôi sẽ viết tắt Gradient Descent là *GD*.

<!--toc-->
# 1. Gradient Descent là gì
Như trong bài [đạo hàm của hàm nhiều biến](/vi/2017/10/multi-var-func/#4-gradient-v%C3%A0-%C4%91%E1%BA%A1o-h%C3%A0m-c%C3%B3-h%C6%B0%E1%BB%9Bng) đã giải thích về gradient và sự biến thiên của hàm số thì hàm số sẽ tăng nhanh nhất theo hướng của gradient (*gradient ascent*) và giảm nhanh nhất theo hướng ngược của gradient (*gradient descent*).

Như vậy một cách trực quan ta có thể nhận xét rằng nếu ta cứ đi ngược hướng đạo hàm mãi thì ta sẽ tới được điểm cực tiểu của hàm số. Nếu bạn cần ví dụ minh họa trực quan thì tôi nghĩ nên xem <a href="https://machinelearningcoban.com/2017/01/12/gradientdescent/" target="_blank"_ rel="noopener noreferrer">bài viết này</a> của anh Tiệp. Theo như tôi thấy thì bài viết của anh ấy rất rõ ràng và đẩy đủ, nên tôi sẽ không phí công viết lại ở đây nữa mà sẽ tập trung vào khai triển cho lập trình.

Giả sử ta cần tìm tham số $\theta\in\mathbb{R}^n$ để tối thiểu hoá hàm lỗi $J(\theta)$. Lúc này giải thuật *GD* được thực hiện bằng cách cập nhập dần các tham số $\theta$ ngược với hướng của gradient $\nabla_\theta J(\theta)$ tại điểm hiện tại cho tới khi nó hội tụ về điểm nhỏ nhất. Tại mỗi bước cập nhập, ta sẽ dịch tham số bằng một lượng $\eta\nabla\_\theta J(\theta)$ với **độ học** (*learning rate*) $\eta>0$ thể hiện cho việc dịch chuyển nhiều tới đâu:
$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla\_\theta J(\theta^{(k)})$$

$\theta^{(k)}$ ở đây kí hiệu cho tham số tại bước cập nhập lần $k$ khi thực hiện giải thuật *GD*.

Ví dụ, ta có hàm $J(\theta) = \theta_0^2+sin(\theta_1)$. Gradient (ma trận Jacobi) lúc này sẽ là:
$$
\nabla\_\theta J(\theta)=
\begin{bmatrix}
\dfrac{\partial}{\partial\theta_0}J\\cr
\dfrac{\partial}{\partial\theta_1}J
\end{bmatrix}=
\begin{bmatrix}
2\theta_0\\cr
\cos(\theta_1)
\end{bmatrix}
$$

Tại bước bất kì các tham số được cập nhập như sau:
$$
\begin{cases}
\theta_0=\theta_0-\eta2\theta_0 \\cr
\theta_1=\theta_1-\eta\cos(\theta_1)
\end{cases}
$$

Lưu ý rằng ta phải cập nhập đồng thời tham số sau khi tính đạo hàm chứ không được đồng thời vừa tính đạo hàm vừa cập nhập tham số. Ví dụ với `Python`:
{{< codeblock "gradient_descent.py" "python">}}
'''
Evaluate Gradient for J = theta_0^2 + sin(theta_1)
'''
def eval_grad(theta):
  # init gradient
  grad = np.empty_like(theta)
  # eval gradient
  grad[0] = 2 * theta[0]
  grad[1] = np.cos(theta[1])
  return grad

# run 1000 times
NUM_INTERS = 1000
# learning rate
ETA = 0.01

# init theta
theta = np.zeros(2)

# run GD
for (i in range(NUM_INTERS)):
  grad = eval_grad(theta)
  theta -= ETA * grad
{{< /codeblock >}}

# 2. Ứng dụng cho hồi quy tuyến tính
Với bài toán hồi quy tuyến tính ta có công thức tính của hàm lỗi như sau:
$$J(\theta)=\frac{1}{2m}\sum\_{i=1}^m(\theta^{\intercal}\phi(\mathbf{x}_i)-y_i)^2$$
Lúc này ta có gradient:
$$\nabla\_\theta J(\theta)=\frac{1}{m}(\theta^{\intercal}\Phi-y)\Phi$$

Trong đó $\Phi$ là [ma trận mẫu](/vi/2017/12/ml-linear-regression/#5-k%E1%BA%BFt-lu%E1%BA%ADn):
$$\Phi=
\begin{bmatrix}
\phi_0(\mathbf{x}_1)&\phi_1(\mathbf{x}_1)&...&\phi\_{n-1}(\mathbf{x}_1)\\cr
\phi_0(\mathbf{x}_2)&\phi_1(\mathbf{x}_2)&...&\phi\_{n-1}(\mathbf{x}_2)\\cr
\vdots&\vdots&\ddots&\vdots\\cr
\phi_0(\mathbf{x}_m)&\phi_1(\mathbf{x}_m)&...&\phi\_{n-1}(\mathbf{x}_m)
\end{bmatrix}
$$

Như vậy tại mỗi bước tham số được cập nhập:
$$
\theta=\theta-\eta\frac{1}{m}(\theta^{\intercal}\Phi-y)\Phi
$$

Code với `Python`:
{{< codeblock "gradient_descent.py" "python" "https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex1.ipynb">}}
def gradient_descent(X, y, theta, eta, num_inters):
    # number of training examples
    m = y.size

    for i in range(num_inters):
        delta = np.dot(np.dot(X, theta) - y, X) / m
        theta -= eta * delta

    return theta
{{< /codeblock >}}

Mã nguồn mẫu đầy đủ bạn có thể xem <a href="https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex1.ipynb" target="_blank"_ rel="noopener noreferrer">tại đây</a> nhé.

# 3. Tính đạo hàm
# 3.1. Tính đạo hàm
Để tính đạo hàm ta có 2 cách:

* **Phương pháp số học** (*numerical gradient*):<br>Phương pháp này sẽ lấy đạo hàm bằng cách tính như định nghĩa đạo hàm. Đạo hàm được định nghĩa là: $f^{\prime}(x)=\lim\limits\_{h\rightarrow 0}\dfrac{f(x+h)-f(x)}{h}$. Nên nếu ta chọn $h$ rất bé thì có thể coi như lấy được đạo hàm gần đúng. Thường người ta lấy $h=\text{1e-5}$ để tính toán. Và trên thực tế thì khi sử dụng phương pháp này ta sẽ sử dụng đạo hàm 2 phía để tính, tức là: $f^{\prime}(x)=\dfrac{f(x+h)-f(x-h)}{2h}$. Phương pháp này khi thực hiện sẽ rất chậm nên thường người ta không sử dụng trong thực tế.
* **Phương pháp giải tích** (*analytic gradient*):<br>Phương pháp này sẽ sử dụng các công thức tính đạo hàm trong giải tích để thực hiện. Ví dụ như đạo hàm của $f(x)=2\sin(x)$ sẽ là $f^{\prime}(x)=2\cos(x)$. Phương pháp này thì nhanh nhưng rất dễ nhầm lẫn nếu công thức tính của ta bị sai.

# 3.2. Kiểm tra đạo hàm
Việc sử dụng phương pháp giải thích có thể dễ bị nhầm khi cài đặt nên thường người ta phải sử dụng cả phương pháp số học để kiểm tra xem việc tính đạo hàm là đúng hay sai. Nếu kết quả tính bằng giải tích và số học không chênh nhau nhiều thì ta có thể tự tin khẳng định việc cài đặt của ta là đúng đắn.

$$|f^{\prime}\_{NG}-f^{\prime}\_{AG}|<\epsilon$$

Trong đó $f^{\prime}\_{NG}$ là đạo hàm tính theo số học và $f^{\prime}\_{AG}$ là đạo hàm tính theo giải thích, còn $\epsilon$ là một số dương rất bé. Thường ta lấy $\epsilon=\text{1e-5}$. Lưu ý là với trường hợp nhiều biến thì ta so sánh đạo hàm riêng của từng biến một nhé.

Do việc tính đạo hàm theo phương pháp số học rất chậm nên thường ta chỉ kiểm tra đạo hàm tại bước cập nhập tham số đầu tiên mà thôi. Tuy nhiễn vẫn có khả năng rủi ro là tính lúc ấy không vấn đề gì nhưng chưa không đảm bảo được chắc chắn rằng việc thực hiện đạo hàm là đúng đắn. Mình đã từng ăn vụ này 1 lần rồi, nên để cho chắc thì kiểm tra ngẫu nhiên vài bước. Chấp nhận đau thương là bị chậm đi còn hơn là đi cả lò bánh!

# 4. Các vấn đề của GD
Ngoài việc dễ nhầm lẫn khi cài đặt GD như vừa đề cập ở trên ra thì vấn đề chọn độ học $\eta$ ra sao cũng rất khó khăn. Nếu $\eta$ quá lớn thì ta không hội tụ được về đích, nhưng nếu $\eta$ quá nhỏ thì ta lại mất rất nhiều thời gian để chạy giải thuật này.

Ngoài ra nếu để ý thì thấy nếu hàm lỗi $J(\theta)$ mà không lồi thì ta rất dễ bị rơi vào điểm tối thiểu cục bộ (*local minimum*) thay vì tiến được tới điểm tối thiểu toàn cục (*global minimum*). Việc chọn $\eta$ lúc này có vai trò rất lớn vì nếu $\eta$ hợp lý thì ta có vể vượt qua được điểm tối ưu cục bộ để tiến tiếp tới điểm tối ưu toán cục.

# 5. Các biến thể
Dạng cài đặt GD từ đầu bài tới giờ được gọi là GD thuần (*Vanilla GD*) hay GD toàn phần (*BGD - Batch GD*). Đặc điểm của nó là sử dụng toàn bộ tập dữ liệu để tính đạo hàm cập nhập tham số nên có thể dẫn tới tình trạng vượt quá khả năng lưu trữ của máy tính và không thể sử dụng được cho các bài toán cần học tức thì (*online training*).

## 5.1. SGD
Thay vì sử dụng toàn bộ tập dữ liệu để cập nhập tham số thì ta có thể sử dụng từng dữ liệu một để cập nhập. Phương pháp như vậy được gọi là **GD ngẫu nhiên** (*SGD - Stochastic Gradient Descent*). Về cơ bản ở mỗi lần cập nhập tham số, ta duyệt toàn bộ các cặp mẫu $(\mathbf{x}^{(i)},y^{(i)})$ và cập nhập tương tự như *BGD* như sau:

$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla\_\theta J(\theta^{(k)};\mathbf{x}^{(i)},y^{(i)})$$

Vì sử dụng từng mẫu đơn một nên tốc độ tính toán đạo hàm sẽ nhanh hơn rất nhiều so với *BGD* nhưng nó phải trả cái giá là tốc độ hội tụ bị giảm đi. Một lưu ý khi cài đặt giải thuật này là mỗi bước cập nhập ta nên xáo trộn dữ liệu rồi mới lấy ra cập nhập. Việc này giúp giảm được sự đi lòng vòng về đích của giải thuật vì ta trao khả năng cập nhập ngẫu nhiên cho nó tức là sẽ có cơ hội nhảy được 1 bước xa hơn khi tính toán.

Code với `Python`:

{{< codeblock "stochastic_gradient_descent.py" "python" >}}
def gradient_descent(X, y, theta, eta, num_inters):
    # number of training examples
    m = y.size

    for i in range(num_inters):
        np.random.shuffle(X)
        for j in X.shape[0]:
            delta = np.dot(np.dot(X[j,:], theta) - y[j], X[j,:]) / m
            theta -= eta * delta

    return theta
{{< /codeblock >}}

## 5.2. Mini-batch GD
Do *SGD* chạy chậm nên người ta thường sử dụng một phương pháp kết hợp giữa *BGD* và *SGD* là sử dụng từng nhóm dữ liệu để cập nhập tham số. Tức là ta sẽ chia dữ liệu ra thành nhiều lô khác nhau và mỗi lần cập nhập dữ liệu, thay vì sử dụng từng mẫu một ta sẽ sử dụng cả lô dữ liệu một. Phương pháp như vậy được gọi là **Mini-batch GD** hay viết tắt là *MGD*. Như vậy ta thấy rằng nếu dữ liệu ta chỉ có 1 lô thì *MGD* chính là *BGD*, nếu mỗi lô chỉ có đúng 1 mẫu thì *MGD* sẽ là *SGD*.

Giả sử lô thứ $i$ được kí hiệu là $(\mathbf{X}^{(i)},\mathbf{y}^{(i)})$, thì công thức cập nhập được viết như sau:

$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla\_\theta J(\theta^{(k)};\mathbf{X}^{(i)},\mathbf{y}^{(i)})$$

Cũng như *SGD* ta cũng sẽ xáo trộn dữ liệu trước khi phân lô cập nhập tham số:

{{< codeblock "mini_batch_gradient_descent.py" "python" >}}
def gradient_descent(X, y, theta, eta, num_inters):
    # number of training examples
    m = y.size

    for i in range(num_inters):
        np.random.shuffle(X)
        while j < X.shape[0]:
            delta = np.dot(np.dot(X[j:j+k,:], theta) - y[j:j+k], X[j:j+k,:]) / m
            theta -= eta * delta

    return theta
{{< /codeblock >}}

# 6. Tăng tốc GD


# 7. Giải thuật cải tiến GD
## 7.1. Momentum
## 7.2. Nesterov accelerated gradient
## 7.3. Adagrad
## 7.4. Adadelta
## 7.5. RMSprop
## 7.6. Adam
## 7.7. AdaMax
## 7.8. Nadam

# 8. Kết luận
