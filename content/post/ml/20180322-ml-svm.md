---
title: "[ML] Support Vector Machine - SVM"
slug: ml-svm
date: 2018-03-22T10:20:14+09:00
categories:
- Học Máy
- ML
tags:
- Học Máy
keywords:
- Học Máy
- Machine Learning
- SVM
- kernel method
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
draft: true
---
Support Vector Machine - **SVM** là một phương pháp học có giám sát trong các mô hình nhận dạng mẫu. Nó không chỉ hoạt động tốt với các dữ liệu được phân tách tuyến tính mà còn tốt với cả dữ liệu phân tách phi tuyến. Với nhiều bài toán, SVM mang lại kết quả tốt như mạng nơ-ron với hiệu quả sử dụng tài nguyên tốt hơn hẳn.
<!--more-->
<!--toc-->

# 1. Phương pháp SVM
Như đã biết, với bài toán phân loại nhị phân tuyến tính ta cần vẽ được mặt phân tách (với không gian 2 chiều thì mặt phẳng này là đường phân tách): $\mathbf{w}^{\intercal}\mathbf{x}+b=0$ để phân biệt được dữ liệu. Khi đó dấu của hàm ước lượng $H=\\{\mathbf{x}	\mapsto\mathrm{sgn}(\mathbf{w}^{\intercal}\mathbf{x}+b)~~~;\mathbf{w}\in\mathbb{R}^N,b\in\mathbb{R}\\}$ sẽ thể hiện được điểm dữ liệu $\mathbf{x}$ nằm ở cụm dữ liệu nào.

{{< image classes="fancybox center" thumbnail-width="100%" src="/images/ml-20180322-svm_1.svg" title="Mặt phân cách dữ liệu" >}}

Nếu để ý thì ta có thể có nhiều mặt phân tách thoả mãn được việc này và đương nhiên là nếu chọn được mặt mà phân tách tốt thì kết quả phân loại của ta sẽ tốt hơn. Một lẽ rất tự nhiên là dường như mặt nằm vừa khít giữa 2 cụm dữ liệu sao cho nằm xa các tập dữ liệu nhất là mặt tốt nhất.

{{< image classes="fancybox center" thumbnail-width="100%" src="/images/ml-20180322-svm_2.svg" title="Max Margin" >}}

**SVM** chính là một biện pháp để thực hiện được phép lấy mặt phẳng như vậy.

Để xác định mặt phẳng kẹp giữa đó, trước tiên ta cần phải xác định được 2 mặt biên gốc như 2 đường nét đứt ở trên. Các điểm dữ liệu gần với mặt biên gốc này nhất có thể xác định bằng:

$$\min{\vert\mathbf{w}^{\intercal}\mathbf{x}+b\vert}$$

Để dễ dàng cho việc tính toán thì người ta sẽ chọn $\mathbf{w}$ và $b$ sao cho các điểm gần nhất (mặt biên gốc) thoả mãn: $\vert\mathbf{w}^{\intercal}\mathbf{x}+b\vert=1$, tức là:
$$\min{\vert\mathbf{w}^{\intercal}\mathbf{x}+b\vert}=1$$

Đương nhiên là có thể tồn tại nhiều cặp đôi mặt biên gốc như vậy và tồn tại nhiều mặt phân đôi kẹp giữa các mặt biên gốc đó. Nên ta phải tìm cách xác định được mặt kẹp giữa tốt nhất bằng cách lấy cặp có khoảng cách xa nhau nhất. Lẽ này là đương nhiên bởi cặp có khoảng cách xa nhất đồng nghĩa với chuyện tập dữ liệu được phân cách xa nhất.

Như vậy, ta có thể thiết lập thông số tính khoảng cách đó bằng phép lấy độ rộng biên từ mặt biên gốc tới mặt phân tách cần tìm.
$$\rho=\min\dfrac{\vert\mathbf{w}^{\intercal}\mathbf{x}+b\vert}{\Vert\mathbf{w}\Vert}=\dfrac{1}{\Vert\mathbf{w}\Vert}$$

Bài toán của ta bây giờ sẽ là cần xác định $\mathbf{w}$ và $b$ sao cho $\rho$ đạt lớn nhất và các điểm dữ liệu $y_i(\mathbf{w}^{\intercal}\mathbf{x}_i+b)\ge 1$. $\rho$ đạt lớn nhất đồng nghĩa với việc $\Vert\mathbf{w}\Vert$ đạt nhỏ nhất. Tức là:
$$
\begin{aligned}
(\mathbf{w},b)&=\arg\min\_{\mathbf{w},b}\frac{1}{2}\Vert\mathbf{w}\Vert^2
\\cr
\text{subject to}~~~&y_i(\mathbf{w}^{\intercal}\mathbf{x}_i+b)\ge 1, i\in[1,m]
\end{aligned}
$$

Ở đây, $m$ là số lượng các điểm dữ liệu $(\mathbf{x}_i,y_i)$ còn việc lấy bình phương và chia đôi nhằm dễ dàng tính toán và tối ưu lồi.

Bài toán này có thể giải thông qua bài toán đối ngẫu của nó và sử dụng phương pháp [nhân tử Lagrance](/vi/2018/02/lagrange-multipliers-2/). Lúc này, ta sẽ cần tìm các giá trị $\lambda$ như sau:
$$
\begin{aligned}
\lambda&=\arg\max_\lambda\sum\_{i=1}^m\lambda_i-\frac{1}{2}\sum\_{i,j=1}^m\lambda_i\lambda_jy_iy_j\mathbf{x}_i^{\intercal}\mathbf{x}_j
\\cr
\text{subject to}~~~&\lambda_i\ge 0 \land \sum\_{i=1}^m\lambda_iy_i=0, i\in[1,m]
\end{aligned}
$$

Việc giải $\lambda$ có thể được thực hiện bằng phương pháp quy hoạch động bậc 2 (*Quadratic Programing*). Với Python ta có thể sử dụng thư viện <a href="http://cvxopt.org/examples/tutorial/qp.html" target="_blank" rel="noopener noreferrer">CVOPT</a>. Sau khi tìm được $\lambda$ thì ta có các tham số:
$$
\begin{aligned}
\mathbf{w}&=\sum\_{i=1}^m\lambda_iy_i\mathbf{x}_i
\\cr
b&=y_i-\sum\_{j=1}^m\lambda_jy_j\mathbf{x}_j^{\intercal}\mathbf{x}_i
\end{aligned}
$$

Ở đây, $(\mathbf{x}_i, y_i)$ là một điểm dữ liệu bất kì nào đó nằm trên đường biên gốc. Điểm dữ liệu này còn được gọi là **Support Vector**. Tên của phương pháp SVM cũng từ đây mà ra. Tuy nhiên, thường người ta tính $b$ bằng phép lấy trung bình tổng của tất cả các $b_i$. Giả sử, ta có tập $\mathbb{S}$ các Support Vectors thì:
$$b=\frac{1}{\vert \mathbb{S}\vert}\sum\_{i\in\mathbb{S}}\Bigg(y_i-\sum\_{j=1}^m\lambda_jy_j\mathbf{x}_j^{\intercal}\mathbf{x}_i\Bigg)$$

Khi đó, một điểm dữ liệu mới sẽ được phân loại dựa theo:
$$h(x)=\mathrm{sgn}\Bigg(\sum\_{i=1}^m\lambda_iy_i\mathbf{x_i}^{\intercal}\mathbf{x}+b\Bigg)$$

Như vậy, chỉ cần các điểm Support Vector trên đường biên gốc là ta có thể ước lượng được các tham số tối ưu cho bài toán. Việc này rất có lợi khi tính toán giúp phương pháp này tiết kiệm được tài nguyên thực thi.

# 2. Dữ liệu chồng nhau và phương pháp biên mềm
Trong thực tế tập dữ liệu thường không được sạch như trên mà thường có nhiễu. Nhiễu ở đây là dạng dữ liệu chồng chéo lên nhau như hình bên dưới.

{{< image classes="fancybox center" thumbnail-width="100%" src="/images/ml-20180322-svm_3.svg" title="Non-linear data" >}}

Với dạng dữ liệu như vậy thì mặt phân tách tìm được sẽ khó mà tối ưu được, thậm chí là không tìm được mặt phân tách luôn. Giờ vấn đề đặt ra là làm sao triệt được các nhiễu này. Tức là tính toán bỏ qua được các nhiễu này khi huấn luyện.


# 3. Dữ liệu phân tách phi tuyến và phương pháp kernel

# 4. Phân tách nhiều lớp

# 5. Kết luận
