---
title: "[NN] Về lan truyền ngược - Backpropagation"
slug: nn-bp
date: 2018-04-27T10:20:14+09:00
categories:
- Học Máy
- ML
tags:
- Học Máy
- NN
keywords:
- Học Máy
- Machine Learning
- Neural Networks
- Backpropagation
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
draft: true
---
Bài viết này được dịch lại từ [bài của anh Christopher Olah](https://colah.github.io/posts/2015-08-Backprop/) bởi anh ấy trình bày rất chi tiết và cực dễ hiểu nên mình không viết lại làm gì cho phí công nữa.
Nội dung của bài viết này không phải về chi tiết giải thuật lan truyền ngược mà viết về nguyên lý cơ bản của giải thuật này. Nếu bạn cần xem chi tiết giải thuật được thực hiện ra sao thì có thể đọc [bài viết trước của tôi](/vi/2018/04/nn-intro/#5-lan-truyền-ngược-và-đạo-hàm).
<!--more-->

<!--toc-->

# 1. Giới thiệu

# 2. Đồ thị tính toán

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png" title="Computational Graph - Tree define" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png" title="Computational Graph - Tree Eval" >}}

# 3. Đạo hàm với đồ thị tính toán

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" title="Computational Graph - Tree Eval Derivative" >}}

# 4. Factoring Paths

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-def-greek.png" title="Chain Define" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-forward-greek.png" title="Chain Forward" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-backward-greek.png" title="Chain Backward" >}}

# 5. Computational Victories

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" title="Tree Eval Derivative" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-forwradmode.png" title="Tree Forward Mode" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-backprop.png" title="Tree Backprop" >}}

# 6. Isn’t This Trivial?
When I first understood what backpropagation was, my reaction was: “Oh, that’s just the chain rule! How did it take us so long to figure out?” I’m not the only one who’s had that reaction. It’s true that if you ask “is there a smart way to calculate derivatives in feedforward neural networks?” the answer isn’t that difficult.

But I think it was much more difficult than it might seem. You see, at the time backpropagation was invented, people weren’t very focused on the feedforward neural networks that we study. It also wasn’t obvious that derivatives were the right way to train them. Those are only obvious once you realize you can quickly calculate derivatives. There was a circular dependency.

Worse, it would be very easy to write off any piece of the circular dependency as impossible on casual thought. Training neural networks with derivatives? Surely you’d just get stuck in local minima. And obviously it would be expensive to compute all those derivatives. It’s only because we know this approach works that we don’t immediately start listing reasons it’s likely not to.

That’s the benefit of hindsight. Once you’ve framed the question, the hardest work is already done.

# 7. Conclusion

Derivatives are cheaper than you think. That’s the main lesson to take away from this post. In fact, they’re unintuitively cheap, and us silly humans have had to repeatedly rediscover this fact. That’s an important thing to understand in deep learning. It’s also a really useful thing to know in other fields, and only more so if it isn’t common knowledge.

Are there other lessons? I think there are.

Backpropagation is also a useful lens for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. The classic example of this is the problem of vanishing gradients in recurrent neural networks.

Finally, I claim there is a broad algorithmic lesson to take away from these techniques. Backpropagation and forward-mode differentiation use a powerful pair of tricks (linearization and dynamic programming) to compute derivatives more efficiently than one might think possible. If you really understand these techniques, you can use them to efficiently calculate several other interesting expressions involving derivatives. We’ll explore this in a later blog post.

This post gives a very abstract treatment of backpropagation. I strongly recommend reading Michael Nielsen’s chapter on it for an excellent discussion, more concretely focused on neural networks.

