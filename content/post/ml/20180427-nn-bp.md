---
title: "[NN] Về lan truyền ngược - Backpropagation"
slug: nn-bp
date: 2018-04-27T10:20:14+09:00
categories:
- Học Máy
- ML
tags:
- Học Máy
- NN
keywords:
- Học Máy
- Machine Learning
- Neural Networks
- Backpropagation
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
draft: true
---
Bài viết này được dịch lại từ [bài của anh Christopher Olah](https://colah.github.io/posts/2015-08-Backprop/) bởi anh ấy trình bày rất chi tiết và cực dễ hiểu nên mình không viết lại làm gì cho phí công nữa.
Nội dung của bài viết này không phải về chi tiết giải thuật lan truyền ngược mà viết về nguyên lý cơ bản của giải thuật này. Nếu bạn cần xem chi tiết giải thuật được thực hiện ra sao thì có thể đọc [bài viết trước của tôi](/vi/2018/04/nn-intro/#5-lan-truyền-ngược-và-đạo-hàm).
<!--more-->

<!--toc-->

# 1. Giới thiệu
Lan truyền ngược (*backpropagation*) là giải thuật cốt lõi giúp cho các mô hình học sâu có thể dễ dàng thực thi tính toán được. Với các mạng NN hiện đại, nhờ giải thuật này mà thuật toán tối ưu với đạo hàm (*gradient descent*) có thể nhanh hơn hàng triệu lần so với cách thực hiện truyền thống. Cứ tưởng tượng 1 mô hình với lan truyền ngược chạy mất 1 tuần thì có thể mất tới 200,000 năm để huấn luyện với phương pháp truyền thống!

Mặc lan truyền ngược được sử dụng cho học sâu, nhưng nó còn là công cụ tính toán mạnh mẽ cho nhiều lĩnh vực khác từ dự báo thời tiết tới phân tích tính ổn định số học, chỉ có điều là nó được sử dụng với những tên khác nhau. Thực ra nó được khai phá lại để sử dụng cho rất nhiều lĩnh vực khác nhau. Nhưng một cách tổng quát không phụ thuộc vào ứng dụng thì tên của nó là "phép vi phân ngược" (*reverse-mode differentiation*).

Về cơ bản, nó là một kĩ thuật để nhanh chóng tính được đạo hàm. Và nó là một mẹo cần thiết mà bạn cần hành trang cho mình không chỉ trong lĩnh vực học sâu mà còn cho nhiều bài toán tính toán số học khac nữa.

# 2. Đồ thị tính toán
Đồ thị tính toán là một cách hay để hiểu các biểu thức toán học. Ví dụ, với biểu thức $e=(a+b)*b+1)$, ta có 3 phép toán: 2 phép cộng và 1 phép nhận. Để dễ giải thích hơn, ta 2 biến $c, d$ để kết quả mỗi phép tính được gán vào một biến nhất định:

$$
\begin{aligned}
c &= a+b
\\cr
d &= b+1
\\cr
e &= c*d
\end{aligned}
$$

To create a computational graph, we make each of these operations, along with the input variables, into nodes. When one node’s value is the input to another node, an arrow goes from one to another.

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png" title="Computational Graph - Tree define" >}}

These sorts of graphs come up all the time in computer science, especially in talking about functional programs. They are very closely related to the notions of dependency graphs and call graphs. They’re also the core abstraction behind the popular deep learning framework Theano.

We can evaluate the expression by setting the input variables to certain values and computing nodes up through the graph. For example, let’s set a=2 and b=1:

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png" title="Computational Graph - Tree Eval" >}}

The expression evaluates to 6.

# 3. Đạo hàm với đồ thị tính toán
If one wants to understand derivatives in a computational graph, the key is to understand derivatives on the edges. If a directly affects c, then we want to know how it affects c. If a changes a little bit, how does c change? We call this the partial derivative of c with respect to a.

To evaluate the partial derivatives in this graph, we need the sum rule and the product rule:

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" title="Computational Graph - Tree Eval Derivative" >}}

# 4. Factoring Paths

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-def-greek.png" title="Chain Define" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-forward-greek.png" title="Chain Forward" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-backward-greek.png" title="Chain Backward" >}}

# 5. Computational Victories

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" title="Tree Eval Derivative" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-forwradmode.png" title="Tree Forward Mode" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-backprop.png" title="Tree Backprop" >}}

# 6. Isn’t This Trivial?
When I first understood what backpropagation was, my reaction was: “Oh, that’s just the chain rule! How did it take us so long to figure out?” I’m not the only one who’s had that reaction. It’s true that if you ask “is there a smart way to calculate derivatives in feedforward neural networks?” the answer isn’t that difficult.

But I think it was much more difficult than it might seem. You see, at the time backpropagation was invented, people weren’t very focused on the feedforward neural networks that we study. It also wasn’t obvious that derivatives were the right way to train them. Those are only obvious once you realize you can quickly calculate derivatives. There was a circular dependency.

Worse, it would be very easy to write off any piece of the circular dependency as impossible on casual thought. Training neural networks with derivatives? Surely you’d just get stuck in local minima. And obviously it would be expensive to compute all those derivatives. It’s only because we know this approach works that we don’t immediately start listing reasons it’s likely not to.

That’s the benefit of hindsight. Once you’ve framed the question, the hardest work is already done.

# 7. Conclusion

Derivatives are cheaper than you think. That’s the main lesson to take away from this post. In fact, they’re unintuitively cheap, and us silly humans have had to repeatedly rediscover this fact. That’s an important thing to understand in deep learning. It’s also a really useful thing to know in other fields, and only more so if it isn’t common knowledge.

Are there other lessons? I think there are.

Backpropagation is also a useful lens for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. The classic example of this is the problem of vanishing gradients in recurrent neural networks.

Finally, I claim there is a broad algorithmic lesson to take away from these techniques. Backpropagation and forward-mode differentiation use a powerful pair of tricks (linearization and dynamic programming) to compute derivatives more efficiently than one might think possible. If you really understand these techniques, you can use them to efficiently calculate several other interesting expressions involving derivatives. We’ll explore this in a later blog post.

This post gives a very abstract treatment of backpropagation. I strongly recommend reading Michael Nielsen’s chapter on it for an excellent discussion, more concretely focused on neural networks.

