---
title: "[NN] Về lan truyền ngược - Backpropagation"
slug: nn-bp
date: 2018-04-27T10:20:14+09:00
categories:
- Học Máy
- ML
tags:
- Học Máy
- NN
keywords:
- Học Máy
- Machine Learning
- Neural Networks
- Backpropagation
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
draft: true
---
Bài viết này được dịch lại từ [bài của anh Christopher Olah](https://colah.github.io/posts/2015-08-Backprop/) bởi anh ấy trình bày rất chi tiết và cực dễ hiểu nên mình không viết lại làm gì cho phí công nữa.
Nội dung của bài viết này không phải về chi tiết giải thuật lan truyền ngược mà viết về nguyên lý cơ bản của giải thuật này. Nếu bạn cần xem chi tiết giải thuật được thực hiện ra sao thì có thể đọc [bài viết trước của tôi](/vi/2018/04/nn-intro/#5-lan-truyền-ngược-và-đạo-hàm).
<!--more-->

<!--toc-->

# 1. Giới thiệu
Lan truyền ngược (*backpropagation*) là giải thuật cốt lõi giúp cho các mô hình học sâu có thể dễ dàng thực thi tính toán được. Với các mạng NN hiện đại, nhờ giải thuật này mà thuật toán tối ưu với đạo hàm (*gradient descent*) có thể nhanh hơn hàng triệu lần so với cách thực hiện truyền thống. Cứ tưởng tượng 1 mô hình với lan truyền ngược chạy mất 1 tuần thì có thể mất tới 200,000 năm để huấn luyện với phương pháp truyền thống!

Mặc lan truyền ngược được sử dụng cho học sâu, nhưng nó còn là công cụ tính toán mạnh mẽ cho nhiều lĩnh vực khác từ dự báo thời tiết tới phân tích tính ổn định số học, chỉ có điều là nó được sử dụng với những tên khác nhau. Thực ra nó được khai phá lại để sử dụng cho rất nhiều lĩnh vực khác nhau. Nhưng một cách tổng quát không phụ thuộc vào ứng dụng thì tên của nó là "phép vi phân ngược" (*reverse-mode differentiation*).

Về cơ bản, nó là một kĩ thuật để nhanh chóng tính được đạo hàm. Và nó là một mẹo cần thiết mà bạn cần hành trang cho mình không chỉ trong lĩnh vực học sâu mà còn cho nhiều bài toán tính toán số học khac nữa.

# 2. Đồ thị tính toán
Đồ thị tính toán là một cách hay để hiểu các biểu thức toán học. Ví dụ, với biểu thức $e=(a+b)*b+1)$, ta có 3 phép toán: 2 phép cộng và 1 phép nhận. Để dễ giải thích hơn, ta 2 biến $c, d$ để kết quả mỗi phép tính được gán vào một biến nhất định:

$$
\begin{aligned}
c &= a+b
\\cr
d &= b+1
\\cr
e &= c*d
\end{aligned}
$$

Để tạo đồ thị tính toán, ta nhóm mỗi phép tính với các biến đầu vào bằng các nút đồ thị. Bằng các mũi tên ta thể hiện đầu ra của một nút là đầu vào cho nút khác.

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-def.png" title="Computational Graph - Tree define" >}}

Kiểu đồ thị thế này rất hay được đề cập trong ngành khoa học máy tính, đặc biệt khi nói tới các chương trình hàm. Chúng rất gần với các kí hiệu của đồ thị phụ thuộc và đồ thị gọi. Chúng cũng là ý tưởng chính của các framework phổ biến như [Theano](http://deeplearning.net/software/theano/) chẳng hạn.

Ta có thể thực hiện các phép toán bằng cách gắn giá trị cho các biến đầu vào bằng các giá trị cụ thể nào đó và tính dần các nút từ dưới lên trên của đồ thị. Ví dụ, gắn $a=2$ và $b=1$, ta được:

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval.png" title="Computational Graph - Tree Eval" >}}

Giá trị thu được là $6$.

# 3. Đạo hàm với đồ thị tính toán
Nếu muốn hiểu đạo hàm trong đồ thị tính toán, thì chìa khoá là cần hiểu được đạo hàm trên các cạnh của đồ thị. Nếu $a$ ảnh hưởng trực tiếp tới $c$ thì ta muốn biết nó ảnh hưởng tới $c$ thế nào. Nếu $a$ thay đổi một chút thì $c$ sẽ thay đổi ra sao? Ta gọi nó là [đạo hàm riêng](https://en.wikipedia.org/wiki/Partial_derivative) của $c$ theo $a$.

Để tính các đạo hàm riêng trên đồ thị này, ta cần phải sử dụng luật cộng và luật nhân:

$$
\begin{aligned}
& \frac{\partial}{\partial{a}}(a+b)=\frac{\partial{a}}{\partial{a}}+\frac{\partial{b}}{\partial{a}}=1
\\cr
& \frac{\partial}{\partial{u}}uv=u\frac{\partial{v}}{\partial{u}}+v\frac{\partial{u}}{\partial{u}}=v
\end{aligned}
$$

Các đạo hàm trên mỗi cạnh được thể hiện trên đồ thị dưới đây:

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" title="Computational Graph - Tree Eval Derivative" >}}

Như vậy, làm sao để biết được một nút bị ảnh hưởng thế nào bởi 1 nút không kết nối trực tiếp? Ví dụ, làm sao để biết $e$ bị ảnh hưởng thế nào bởi $a$. Nếu ta thay đổi $a$ 1 đơn vị, thì $c$ cũng thay đổi 1 đơn vị. $c$ thay đổi 1 đơn vị thì $e$ bị thay đổi 2 đơn vị. Vì vậy, $e$ thay đổi $1*2$ đơn vị theo sự thay đổi của $a$.

Quy tắc chung là lấy tổng tất cả các đường từ một nút tới nút khác và nhân với đạo hàm trên mỗi cạnh tương ứng. Ví dụ, để tính đạo hàm cùa $e$ theo $b$, ta có:

$$
\frac{\partial{e}}{\partial{b}}=1*2+1*3
$$

Điều này có nghĩa rằng $b$ ảnh hưởng thế nào tới $e$ thông qua $c$ ($\dfrac{\partial{e}}{\partial{c}}\dfrac{\partial{c}}{\partial{b}}$) và $d$ ($\dfrac{\partial{e}}{\partial{d}}\dfrac{\partial{d}}{\partial{b}}$).

# 4. Factoring Paths

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-def-greek.png" title="Chain Define" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-forward-greek.png" title="Chain Forward" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/chain-backward-greek.png" title="Chain Backward" >}}

# 5. Computational Victories

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png" title="Tree Eval Derivative" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-forwradmode.png" title="Tree Forward Mode" >}}

{{< image classes="fancybox center" thumbnail-width="60%" src="https://colah.github.io/posts/2015-08-Backprop/img/tree-backprop.png" title="Tree Backprop" >}}

# 6. Isn’t This Trivial?
When I first understood what backpropagation was, my reaction was: “Oh, that’s just the chain rule! How did it take us so long to figure out?” I’m not the only one who’s had that reaction. It’s true that if you ask “is there a smart way to calculate derivatives in feedforward neural networks?” the answer isn’t that difficult.

But I think it was much more difficult than it might seem. You see, at the time backpropagation was invented, people weren’t very focused on the feedforward neural networks that we study. It also wasn’t obvious that derivatives were the right way to train them. Those are only obvious once you realize you can quickly calculate derivatives. There was a circular dependency.

Worse, it would be very easy to write off any piece of the circular dependency as impossible on casual thought. Training neural networks with derivatives? Surely you’d just get stuck in local minima. And obviously it would be expensive to compute all those derivatives. It’s only because we know this approach works that we don’t immediately start listing reasons it’s likely not to.

That’s the benefit of hindsight. Once you’ve framed the question, the hardest work is already done.

# 7. Conclusion

Derivatives are cheaper than you think. That’s the main lesson to take away from this post. In fact, they’re unintuitively cheap, and us silly humans have had to repeatedly rediscover this fact. That’s an important thing to understand in deep learning. It’s also a really useful thing to know in other fields, and only more so if it isn’t common knowledge.

Are there other lessons? I think there are.

Backpropagation is also a useful lens for understanding how derivatives flow through a model. This can be extremely helpful in reasoning about why some models are difficult to optimize. The classic example of this is the problem of vanishing gradients in recurrent neural networks.

Finally, I claim there is a broad algorithmic lesson to take away from these techniques. Backpropagation and forward-mode differentiation use a powerful pair of tricks (linearization and dynamic programming) to compute derivatives more efficiently than one might think possible. If you really understand these techniques, you can use them to efficiently calculate several other interesting expressions involving derivatives. We’ll explore this in a later blog post.

This post gives a very abstract treatment of backpropagation. I strongly recommend reading Michael Nielsen’s chapter on it for an excellent discussion, more concretely focused on neural networks.

