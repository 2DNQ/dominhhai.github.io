---
title: "[RNN] Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm"
slug: understand-rnn-bptt
date: 2017-10-22
categories:
- Học Máy
- Học Sâu
- RNN
tags:
- RNN
keywords:
- Mạng RNN
- Học Sâu
- Deep Learning
autoThumbnailImage: true
thumbnailImagePosition: left
thumbnailImage: //res.cloudinary.com/dominhhai/image/upload/dl/logo.png
metaAlignment: center
draft: true
---
> Bài giới thiệu RNN thứ 3 này được dịch lại từ trang <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank">blog WILDML</a>.

Trong phần này tôi sẽ giới thiệu tổng quan về BPTT (Backpropagation Through Time) và giải thích sự khác biệt của nó so với các giải thuật lan truyền ngược truyền thống.
Sau đó ta sẽ cùng tìm hiểu vấn đề mất mát đạo hàm (vanishing gradient problem), nó dẫn ta tới việc phát triển của LSTM và GRU - 2 mô hình phổ biến và mạnh mẽ nhất hiện nay trong các bài toán NLP (và cả các lĩnh vực khác).

<!--more-->

Vấn đề mất mát đạo hàm được khám phá bởi <a href="http://people.idsia.ch/~juergen/fundamentaldeeplearningproblem.html" target="_blank">Sepp Hochreiter năm 1991</a>
và đã cuốn hút được sự quan tâm cho lần nữa trong thời gian gần đây khi mà ứng dụng của các kiến trúc sâu ngày một nhiều hơn.

Đây là bài thứ 3 trong chuỗi bài giới thiệu về RNN:

* 1. [Giới thiệu RNN](/vi/2017/10/what-is-rnn/)
* 2. [Cài đặt RNN với Python và Theano](/vi/2017/10/implement-rnn-with-python/)
* 3. Tìm hiểu về giải thuật BPTT và vấn đề mất mát đạo hàm (bài này)
* 4. [Cài đặt GRU/LSTM](/vi/2017/10/implement-gru-lstm/)

Để có thể hiểu được toàn bộ bài viết này, bạn cần có kiến thức về đạo hàm riêng và cơ bản về giải thuật lan truyền ngược (backpropagation).
Nếu bạn chưa rõ nó thì có thể đọc tại các bài viết
<a href="http://cs231n.github.io/optimization-2/" target="_blank">này</a>
và <a href="http://colah.github.io/posts/2015-08-Backprop/" target="_blank">này</a>
và cả <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank">đây</a> nữa (thứ tự khó dần nhé).

<!-- toc -->

# 1. Lan truyền ngược liên hồi - BPTT
Nhớ lại chút các công thức cơ bản của RNN. Lưu ý rằng các kí hiệu ở đây có thay đổi 1 chút từ $ o $ thành $ \hat{y} $.
Việc thay đổi này nhằm thống nhất với một vài tài liệu tôi sẽ tham chiếu tới.

$$
\begin{aligned}
s_t &= tanh(U x_t + W s\_{t-1}) \\cr
\hat{y_t} &= softmax(V s_t)
\end{aligned}
$$

Ta cũng định nghĩa hàm mất mát, hay hàm lỗi dạng cross entropy như sau:

$$
\begin{aligned}
E_t(y_t, \hat{h_y}) &= -y_t log{\hat{y_t}} \\cr
E(y, \hat(y)        &= \sum_t{E_t(y_t, \hat{h_y})} \\cr
. &= -\sum_t{y_t log{h_y})}
\end{aligned}
$$

{{< image classes="fancybox center" src="//d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/rnn-bptt1.png" >}}

{{< image classes="fancybox center" src="//d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png" >}}


{{< codeblock "bptt.py" "python" >}}
def bptt(self, x, y):
    T = len(y)
    # Perform forward propagation
    o, s = self.forward_propagation(x)
    # We accumulate the gradients in these variables
    dLdU = np.zeros(self.U.shape)
    dLdV = np.zeros(self.V.shape)
    dLdW = np.zeros(self.W.shape)
    delta_o = o
    delta_o[np.arange(len(y)), y] -= 1.
    # For each output backwards...
    for t in np.arange(T)[::-1]:
        dLdV += np.outer(delta_o[t], s[t].T)
        # Initial delta calculation: dL/dz
        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))
        # Backpropagation through time (for at most self.bptt_truncate steps)
        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:
            # print "Backpropagation step t=%d bptt step=%d " % (t, bptt_step)
            # Add to gradients at each previous step
            dLdW += np.outer(delta_t, s[bptt_step-1])              
            dLdU[:,x[bptt_step]] += delta_t
            # Update delta for next step dL/dz at t-1
            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)
    return [dLdU, dLdV, dLdW]
{{< /codeblock >}}
# 2. Vấn đề mất mát đạo hàm


{{< image classes="fancybox center" src="//nn.readthedocs.org/en/rtd/image/tanh.png" title="tanh and derivative. Source: http://nn.readthedocs.org/en/rtd/transfer/" >}}
