<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.31.1 with theme Tranquilpeak 0.4.1-BETA"><title>[ML] MLE của hồi quy tuyến tính</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><meta name=description content="Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (mean squared error). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng MLE (Maximum Likelihood Esitmation) xem sao."><meta property=og:type content=website><meta property=og:title content="[ML] MLE của hồi quy tuyến tính"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><meta property=og:description content="Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (mean squared error). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng MLE (Maximum Likelihood Esitmation) xem sao."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[ML] MLE của hồi quy tuyến tính"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><meta name=twitter:description content="Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (mean squared error). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng MLE (Maximum Likelihood Esitmation) xem sao."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-fpbzgxsy0kgmdvyrj5ykkg6ratccrk3gocmaqn4xpcjywmv5dteilzucro4f.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.css integrity=sha384-8QOKbPtTFvh/lMY0qPVbXj9hDh+v8US0pD//FcoYFst2lCIf0BmT58+Heqj0IGyx><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[ML] MLE của hồi quy tuyến tính</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-21T12:28:26&#43;09:00>21 tháng 12, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (<em>mean squared error</em>). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle>MLE (<em>Maximum Likelihood Esitmation</em>)</a> xem sao.<h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-mô-phỏng-xác-suất>1. Mô phỏng xác suất</a></li><li><a href=#2-mle-với-phân-phối>2. MLE với phân phối</a><ul><li><a href=#2-1-mle-với-phân-phối-chuẩn>2.1. MLE với phân phối chuẩn</a></li><li><a href=#2-2-mle-với-phân-phối-có-nhiễu-dạng-chuẩn>2.2. MLE với phân phối có nhiễu dạng chuẩn</a></li></ul></li><li><a href=#3-mle-cho-hồi-quy-tuyến-tính>3. MLE cho hồi quy tuyến tính</a></li><li><a href=#4-kết-luận>4. Kết luận</a></li></ul></nav></p><h1 id=1-mô-phỏng-xác-suất>1. Mô phỏng xác suất</h1><p>Theo định lý <a href=https://en.wikipedia.org/wiki/Central_limit_theorem target=_blank _ rel="noopener noreferrer">giới hạn trung tâm</a> (<em>central limit theorem</em>) thì phân phối xác suất của biến ngẫu nhiên sẽ hội tụ về phân phối chuẩn. Vận dụng định lý này cho đầu ra của mỗi mô hình hồi quy tuyến tính, ta sẽ thêm 1 lượng nhiễu theo xác suất chuẩn vào đầu ra, ta sẽ được:
$$t=y(\mathbf{x},\theta)+\mathcal{N}(0,\sigma^2) ~~~,(1.1)$$</p><p>Như đã phân tích ở <a href=https://dominhhai.github.io/vi/2017/10/prob-com-var/#2-2-1-%C4%91%E1%BB%91i-v%E1%BB%9Bi-bi%E1%BA%BFn-1-chi%E1%BB%81u-univariate>phần phân phối chuẩn</a> ta có thể biểu diễn phân phối của $t$ bằng phân phối chuẩn:
$$p(t|\mathbf{x},\theta,\sigma)=\mathcal{N}(t|y(\mathbf{x},\theta),\sigma^2)$$</p><p>Đặt $\beta=\dfrac{1}{\sigma^2}$, ta có:
$$p(t|\mathbf{x},\theta,\beta)=\mathcal{N}(t|y(\mathbf{x},\theta),\beta^{-1})$$</p><p>Do $y(\mathbf{x},\theta)=\theta^{\intercal}\phi(\mathbf{x})$, nên:
$$p(t|\mathbf{x},\theta,\beta)=\mathcal{N}(t|\theta^{\intercal}\phi(\mathbf{x}),\beta^{-1})$$</p><p>Với giả sử dữ liệu huấn luyện của ta là I.I.D (mẫu ngẫu nhiên), ta sẽ thu được xác suất toàn mẫu là:
$$p(\mathbf{t}|\mathbf{X},\theta,\beta)=\prod_{i=1}^m\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})$$</p><p>Trong đó $\mathbf{t}=[t_1,t_2,&hellip;,t_m]^{\intercal}$ và $\mathbf{X}=[\mathbf{x}_1, \mathbf{x}_2,&hellip;,\mathbf{x}_m]^{\intercal}$ lần lượt là đầu ra và đầu vào thực tế (dữ liệu từ tập huấn luyện).</p><h1 id=2-mle-với-phân-phối>2. MLE với phân phối</h1><p>Trước tiên ta sẽ xét MLE cho phân phối chuẩn một cách tổng quát rồi sẽ đi vào bài toán hồi quy tuyến tính. Vì việc nắm được lý thuyết sẽ giúp phân tích trường hợp cụ thể đơn giản hơn. Ở đây tôi không nhắc lại MLE là gì nữa mà sẽ đi thẳng vào vấn đề luôn. Nếu bạn cần tìm hiểu MLE là gì thì có thể xem tại <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle target=_blank _>bài viết này</a>.</p><h2 id=2-1-mle-với-phân-phối-chuẩn>2.1. MLE với phân phối chuẩn</h2><p>Giả sử rằng ta có $\theta=[\mu,\sigma^2]$ và tập mẫu biến ngẫu nhiên $\mathbf{X}=[X_1,X_2,&hellip;,X_m]$ tuân theo phân phối chuẩn: $X_i\sim\mathcal{N}(\mu,\sigma^2)$. Giờ nhiệm vụ của ta là phải tìm được các tham số $\theta$ để phân phối toàn mẫu đạt lớn nhất có thể.</p><p>Ta có xác suất toàn mẫu là:
$$
\begin{aligned}
L(\theta)&amp;=\prod_{i=1}^mf(X_i|\theta)
\cr\ &amp;=\prod_{i=1}^m\frac{1}{\sqrt{2\pi\theta_1}}\exp\Bigg(-\frac{(X_i-\theta_0)^2}{2\theta_1}\Bigg)
\end{aligned}
$$</p><p>Lấy log ta sẽ được:
$$
\begin{aligned}
LL(\theta)&amp;=\sum_{i=1}^m\log\frac{1}{\sqrt{2\pi\theta_1}}\exp\Bigg(-\frac{(X_i-\theta_0)^2}{2\theta_1}\Bigg)
\cr\ &amp;=\sum_{i=1}^m\Big(-\log\sqrt{2\pi\theta_1}-\frac{(X_i-\theta_0)^2}{2\theta_1}\Big)
\cr\ &amp;=-\frac{m}{2}\log(2\pi\theta_1)-\frac{1}{2\theta_1}\sum_{i=1}^m(X_i-\theta_0)^2
\cr\ &amp;=-\frac{m}{2}\log(2\pi)-\frac{m}{2}\log(\theta_1)-\frac{1}{2\theta_1}\sum_{i=1}^m(X_i-\theta_0)^2
\end{aligned}
$$</p><p>Để tìm tham số $\theta$ cho hàm $LL(\theta)$ trên đạt cực đại, ta sẽ sử dụng đạo hàm để giải quyết.</p><p>Với tham số $\theta_0$, đạo hàm riêng sẽ là:
$$\frac{\partial{LL}}{\theta_0}=\frac{1}{\theta_1}\sum_{i=1}^m(X_i-\theta_0)$$
Khi đạt cực đại, đạo hàm bị triệt tiêu ta sẽ có:
$$
\begin{aligned}
\ &amp;\frac{1}{\theta_1}\sum_{i=1}^m(X_i-\theta_0)=0
\cr\iff &amp;m\theta_0=\sum_{i=1}^mX_i
\cr\iff &amp;\theta_0=\frac{1}{m}\sum_{i=1}^mX_i
\end{aligned}
$$</p><p>Tương tự, đạo hàm theo $\theta_1$ triệt tiêu:
$$
\begin{aligned}
\ &amp;\frac{\partial{LL}}{\theta_1}=0
\cr\iff &amp;-\frac{m}{2\theta_1}+\frac{1}{2\theta_1^2}\sum_{i=1}^m(X_i-\theta_0)^2=0
\cr\iff &amp;m\theta_1=\sum_{i=1}^m(X_i-\theta_0)^2
\cr\iff &amp;\theta_1=\frac{1}{m}\sum_{i=1}^m(X_i-\theta_0)^2
\end{aligned}
$$</p><p>Như vậy giá tham số ước lượng được là $\hat\theta_0=\hat\mu=\dfrac{1}{m}\displaystyle\sum_{i=1}^mX_i$ và $\hat\theta_1=\hat\sigma^2=\dfrac{1}{m}\displaystyle\sum_{i=1}^m(X_i-\theta_0)^2$.</p><h2 id=2-2-mle-với-phân-phối-có-nhiễu-dạng-chuẩn>2.2. MLE với phân phối có nhiễu dạng chuẩn</h2><p>Giả sử biến ngẫu nhiên $Y=\theta X+\mathcal{N}(0,\sigma^2)$ với phương sai $\sigma^2$ là cố định (biên dao động của nhiễu không đổi) và ta chưa biết phân phối của $X$ thế nào cả. Như vậy thì nếu $X$ đã biết trước $Y$ cũng tuân theo phân phối chuẩn $Y|X\sim\mathcal{N}(\theta X,\sigma^2)$. Nhiệm vụ của ta là ước lượng tham số $\theta$ sao cho xác suất của mẫu ngẫu nhiên đạt lớn nhất.</p><p>Giả sử ta có mẫu ngẫu nhiên có $m$ cặp dữ liệu $\{(X_1,Y_1),(X_2,Y_2),&hellip;,(X_m,Y_m)\}$, xác suất hợp của toàn mẫu sẽ là:</p><p>$$
\begin{aligned}
L(\theta)&amp;=\prod_{i=1}^mf(Y_i,X_i|\theta)
\cr\ &amp;=\prod_{i=1}^mf(Y_i|X_i,\theta)f(X_i|\theta)
\cr\ &amp;=\prod_{i=1}^mf(Y_i|X_i,\theta)f(X_i)
\cr\ &amp;=\prod_{i=1}^m\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Bigg(-\frac{(Y_i-\theta X_i)^2}{2\sigma^2}\Bigg)f(X_i)
\end{aligned}
$$</p><p>Lấy log ta được:
$$
\begin{aligned}
LL(\theta)&amp;=\sum_{i=1}^m\log\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Bigg(-\frac{(Y_i-\theta X_i)^2}{2\sigma^2}\Bigg)f(X_i)
\cr\ &amp;=-\frac{m}{2}\log(2\pi)-\frac{m}{2}\log(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^m(Y_i-\theta X_i)^2+\sum_{i=1}^m\log f(X_i)
\end{aligned}
$$</p><p>Nếu đặt $\beta=\dfrac{1}{\sigma^2}$ thì công thức trên sẽ thành:
$$LL(\theta)=\frac{m}{2}\log\beta-\frac{m}{2}\log(2\pi)-\frac{1}{2}\beta\sum_{i=1}^m(Y_i-\theta X_i)^2+\sum_{i=1}^m\log f(X_i)$$</p><p>Giờ để ước lượng $\theta$ sao cho $LL(\theta)$ đạt cực đại thì ta chỉ cần quan tâm tới thành phần có $\theta$ tức là việc ước lượng thành:
$$\hat\theta=\arg\min_\theta\sum_{i=1}^m(Y_i-\theta X_i)^2$$</p><h1 id=3-mle-cho-hồi-quy-tuyến-tính>3. MLE cho hồi quy tuyến tính</h1><p>Giờ áp dụng MLE ta cần tìm tham số để cho xác suất toàn mẫu là lớn nhất có thể:
$$
\begin{aligned}
(\hat\theta,\hat\beta)&amp;=\arg\max_{\theta,\beta}\prod_{i=1}^m\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})
\cr\ &amp;=\arg\max_{\theta,\beta}\sum_{i=1}^m\log\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})
\end{aligned}
$$</p><p>Như phân tích ở trên ta đã có:
$$\sum_{i=1}^m\log\mathcal{N}(t_i|\theta^{\intercal}\phi(\mathbf{x}_i),\beta^{-1})=\frac{m}{2}\beta-\frac{m}{2}\log(2\pi)-\frac{1}{2}\beta\sum_{i=1}^m\Big(t_i-\theta^{\intercal}\phi(\mathbf{x}_i)\Big)^2$$</p><p>Ở đây tôi lược bỏ thành phần $X$ đi để cho đơn giản. Thế giờ bạn nhìn thấy hàm lỗi $J(\theta)$ chưa?
$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m\Big(t_i-\theta^{\intercal}\phi(\mathbf{x}_i)\Big)^2$$</p><p>Từ đây ta sẽ được:
$$
\begin{aligned}
(\hat\theta,\hat\beta)&amp;=\arg\max_{\theta,\beta}\Big(\frac{m}{2}\beta-\frac{m}{2}\log(2\pi)-m\beta J(\theta)\Big)
\cr\ &amp;=\arg\max_{\theta,\beta}\beta\Big(1-2J(\theta)\Big)
\end{aligned}
$$</p><p>Nếu coi $\beta$ ở đây là cố định (các điểm đầu ra có mức dao động như nhau) thì việc cực đại hoá này được quy về việc cực tiểu hoá hàm lỗi $J(\theta)$:
$$\hat\theta=\arg\min_\theta J(\theta)$$</p><h1 id=4-kết-luận>4. Kết luận</h1><p>Qua quá trình phân tích này ta nhận thấy được sự tương đồng giữa việc tối thiểu hoá hàm lỗi và cực đại hoá độ hợp lý tham số. Trên cơ sở đó ta hoàn toàn có thể yên tâm về mức độ tin cậy của phương pháp tối ưu hàm lỗi của ta.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-gd/ data-tooltip="[ML] Tối ưu hàm lỗi với Gradient Descent"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Hồi quy tuyến tính (Linear Regression)"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2017 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-gd/ data-tooltip="[ML] Tối ưu hàm lỗi với Gradient Descent"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/ data-tooltip="[ML] Hồi quy tuyến tính (Linear Regression)"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-linear-regression-mle%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-linear-regression-mle%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-linear-regression-mle%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank rel=noopener class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input id=algolia-search-input name=search class="form-control input--large search-input" placeholder="Tìm kiếm"></form></div><div class=modal-body><div class="no-result text-color-light text-center">không tìm thấy kết quả</div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-gd/><h3 class=media-heading>[ML] Tối ưu hàm lỗi với Gradient Descent</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là <strong>Gradient Descent</strong> thường được sử dụng.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><h3 class=media-heading>[ML] MLE của hồi quy tuyến tính</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (<em>mean squared error</em>). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle>MLE (<em>Maximum Likelihood Esitmation</em>)</a> xem sao.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/><h3 class=media-heading>[ML] Hồi quy tuyến tính (Linear Regression)</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Học có giám sát (<em>Supervised Learning</em>) được chia ra làm 2 dạng lớn là <strong>hồi quy</strong> (<em>regression</em>) và <strong>phân loại</strong> (<em>classification</em>) dựa trên tập dữ liệu mẫu - tập huấn luyện (<em>training data</em>). Với bài đầu tiên này ta sẽ bắt đầu bằng bài toán hồi quy mà cụ thể là hồi quy tuyến tính (<em>linear regression</em>).</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-prml/><h3 class=media-heading>Pattern Recognition and Machine Learning</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-intro/><h3 class=media-heading>[ML] Học máy là gì?</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thời gian gần đây AI nổi lên mạnh mẽ xâm nhập vào rất nhiều lĩnh vực trong cuộc sống như tự động dịch thuật, nhận dạng giọng nói, điều khiển tự động, v.v. Nó giờ được coi là xu hướng công nghệ thế giới và nhiều người cho rằng đó là cuộc cách mạng công nghiệp lần thứ 4.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/seiko-no-yotei/><h3 class=media-heading>Điểm cốt lõi để thành công</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather">Cuốn sách tổng hợp nội dung của một số buổi nói chuyện của ông Inamori Kazuo về kinh doanh, làm việc, nhân sinh. Vẫn phong cách quen thuộc về cách nhìn cuộc sống, cách suy nghĩ, cách hành động như trong các cuốn sách khác mà ông đã viết, nhưng trong cuốn này đặc biệt ở chỗ tổng hợp được nhiều nội dung khá cô động mà vẫn không thiếu sót nội dung.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/vbnet-oracle-version/><h3 class=media-heading>[.NET] Sài nhiều phiên bản Oracle khi thực thi</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thông thường khi ta build ứng dụng thì phiên bản Oracle DB ở môi trường phát triển và môi trường thực thi là giống nhau nên không xảy ra vấn đề gì cả. Nhưng nếu ở môi trường phát triển và thực thi khác nhau thì sao?</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/what-is-http2/><h3 class=media-heading>[Web] HTTP2 là gì?</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Nhân tiện bản <code>Node v9x</code> mới ra cho phép ta có thể sử dụng ngay API thử nghiệm <code>HTTP/2</code> nên cũng tò mò tìm hiểu đôi chút xem kiến trúc, đặc điểm và cách sử dụng thế nào.
Sau 2 năm ra chính thức ra lò, phiên bản tiếp theo của <code>HTTP</code> này dần được nhiều máy chủ Web lẫn trình duyệt hỗ trợ bởi tính vượt trội của nó so với phiên bản <code>HTTP/1.1</code>.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/about-git/><h3 class=media-heading>[Git] Mô tả về GIT của Linus Torvalds</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Đây là mô tả về GIT mà chủ nhân của nó - ông Linus Torvalds đã viết khi công khai mã nguồn. Cụ thể bài này được copy lại từ <a href=https://github.com/git/git/tree/e83c5163316f89bfbde7d9ab23ca2e25604af290 target=_blank _ rel="noopener noreferrer">Github</a>.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/><h3 class=media-heading>[Windows] Đổi tên file với .bat file</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather">Gần đây Gmail không cho phép gửi các file có đuôi là mã nguồn ngôn ngữ lập trình như .js, .vb chẳng hạn. Ngay cả việc đổi đuôi của các file nén cũng không có hiệu quả như trước, nên buộc phải tìm cách đổi toàn bộ đuôi 1 phát.
Bài viết này sẽ nói về cách thay đổi toàn bộ đuôi file bằng .bat file của Windows, tuy nhiên hoàn toàn có thể sử dụng để làm những chuyện khác với các file này như đổi tên chẳng hạn.</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero="không tìm thấy kết quả" data-message-one="tìm thấy 1 kết quả" data-message-other="tìm thấy {n} kết quả">55 posts found</p></div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js integrity="sha256-IFHWFEbU2/+wNycDECKgjIRSirRNIDp2acEB5fvdVRU=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js integrity="sha256-+mpyNVJsNt4rVXCw0F+pAOiB3YxmHgrbJsx4ecPuUaI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js integrity="sha256-vMxgR/7FtLovVA+IPrR7+xTgIgARH7y9VZQnmmi0HDI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js integrity="sha256-N0qFUh7/9vLvia87dDndewmsgsyYoNkdA212tPc+2NI=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-GR8SEkOO1rBN/jnOcQDFcFmwXAevSLx7/Io9Ps1rkxWp983ZIuUGfxivlF/5f5eJ src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.js></script><script crossorigin=anonymous integrity=sha384-cXpztMJlr2xFXyDSIfRWYSMVCXZ9HeGXvzyKTYrn03rsMAlOtIQVzjty5ULbaP8L src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-linear-regression-mle\/';this.page.identifier='\/vi\/2017\/12\/ml-linear-regression-mle\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>