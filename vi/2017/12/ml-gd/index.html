<!doctype html><html lang=vi><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=generator content="Hugo 0.31.1 with theme Tranquilpeak 0.4.1-BETA"><title>[ML] Tối ưu hàm lỗi với Gradient Descent</title><meta name=author content="Do Minh Hai"><meta name=keywords content="Học Máy,Machine Learning,dominhhai,programming,computer science,machine learning,deep learning"><link rel=icon href=https://dominhhai.github.io/favicon/golden-buddha-512-79567.png><link rel=canonical href=https://dominhhai.github.io/vi/2017/12/ml-gd/><meta name=description content="Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là Gradient Descent thường được sử dụng."><meta property=og:type content=website><meta property=og:title content="[ML] Tối ưu hàm lỗi với Gradient Descent"><meta property=og:url content=https://dominhhai.github.io/vi/2017/12/ml-gd/><meta property=og:description content="Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là Gradient Descent thường được sử dụng."><meta property=og:site_name content="Hai's Blog"><meta property=og:locale content=vi><meta name=twitter:card content=summary><meta name=twitter:title content="[ML] Tối ưu hàm lỗi với Gradient Descent"><meta name=twitter:url content=https://dominhhai.github.io/vi/2017/12/ml-gd/><meta name=twitter:description content="Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là Gradient Descent thường được sử dụng."><meta name=twitter:creator content=@minhhai3b><meta property=og:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta name=twitter:image content="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=640"><meta property=og:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><meta name=twitter:image content=https://res.cloudinary.com/dominhhai/image/upload/dl/logo.png><link rel=publisher href=https://plus.google.com/115106277658014197977><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin=anonymous><link rel=stylesheet href=https://dominhhai.github.io/css/style-fpbzgxsy0kgmdvyrj5ykkg6ratccrk3gocmaqn4xpcjywmv5dteilzucro4f.min.css><link rel=stylesheet crossorigin=anonymous href=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.css integrity=sha384-8QOKbPtTFvh/lMY0qPVbXj9hDh+v8US0pD//FcoYFst2lCIf0BmT58+Heqj0IGyx><link rel=stylesheet href=https://dominhhai.github.io/css/main.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-105333519-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-105333519-1');</script></head><body><div id=blog><header id=header data-behavior=5><i id=btn-open-sidebar class="fa fa-lg fa-bars"></i><div class=header-title><a class=header-title-link href=https://dominhhai.github.io/vi/>Hai&#39;s Blog</a></div><a class=header-right-picture href=https://dominhhai.github.io/#about><img class=header-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=90" alt="Ảnh đại diện"></a></header><nav id=sidebar data-behavior=5><div class=sidebar-container><div class=sidebar-profile><a href=https://dominhhai.github.io/#about><img class=sidebar-profile-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"></a><h4 class=sidebar-profile-name>Do Minh Hai</h4><h5 class=sidebar-profile-bio>Just a developer<br>Enjoy life as a journey</h5></div><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/><i class="sidebar-button-icon fa fa-lg fa-home"></i><span class=sidebar-button-desc>Trang chủ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/categories/><i class="sidebar-button-icon fa fa-lg fa-bookmark"></i><span class=sidebar-button-desc>Danh mục</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/tags/><i class="sidebar-button-icon fa fa-lg fa-tags"></i><span class=sidebar-button-desc>Thẻ thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/archives/><i class="sidebar-button-icon fa fa-lg fa-archive"></i><span class=sidebar-button-desc>Lưu trữ</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/#about><i class="sidebar-button-icon fa fa-lg fa-address-card"></i><span class=sidebar-button-desc>Thông tin</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/page/why/><i class="sidebar-button-icon fa fa-lg fa-question"></i><span class=sidebar-button-desc>Hỏi ngu</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://github.com/dominhhai target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-github"></i><span class=sidebar-button-desc>GitHub</span></a></li><li class=sidebar-button><a class=sidebar-button-link href=https://twitter.com/minhhai3b target=_blank rel=noopener><i class="sidebar-button-icon fa fa-lg fa-twitter"></i><span class=sidebar-button-desc>Twitter</span></a></li></ul><ul class=sidebar-buttons><li class=sidebar-button><a class=sidebar-button-link href=https://dominhhai.github.io/vi/index.xml><i class="sidebar-button-icon fa fa-lg fa-rss"></i><span class=sidebar-button-desc>RSS</span></a></li></ul></div></nav><div id=main data-behavior=5 class=hasCoverMetaIn><article class=post itemscope itemtype=http://schema.org/BlogPosting><div class="post-header main-content-wrap text-center"><h1 class=post-title itemprop=headline>[ML] Tối ưu hàm lỗi với Gradient Descent</h1><div class="postShorten-meta post-meta"><time itemprop=datePublished datetime=2017-12-22T08:45:04&#43;09:00>22 tháng 12, 2017</time>
<span>mục</span>
<a class=category-link href=https://dominhhai.github.io/vi/categories/h%e1%bb%8dc-m%c3%a1y>Học Máy</a>,
<a class=category-link href=https://dominhhai.github.io/vi/categories/ml>ML</a></div></div><div class="post-content markdown" itemprop=articleBody><div class=main-content-wrap><p>Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là <strong>Gradient Descent</strong> thường được sử dụng.</p><p>Chính vì vậy tôi viết bài này để hiểu rõ hơn về phương pháp tối ưu bằng giải thuật <strong>Gradient Descent</strong> cũng như các biến thể và tối ưu cho giải thuật này. Để cho thuận tiện từ giờ trở đi tôi sẽ viết tắt Gradient Descent là <em>GD</em>.</p><h1 id=table-of-contents>Mục lục</h1><nav id=TableOfContents><ul><li><a href=#1-gradient-descent-là-gì>1. Gradient Descent là gì</a></li><li><a href=#2-ứng-dụng-cho-hồi-quy-tuyến-tính>2. Ứng dụng cho hồi quy tuyến tính</a></li><li><a href=#3-tính-đạo-hàm>3. Tính đạo hàm</a><ul><li><a href=#3-1-tính-đạo-hàm>3.1. Tính đạo hàm</a></li><li><a href=#3-2-kiểm-tra-đạo-hàm>3.2. Kiểm tra đạo hàm</a></li></ul></li><li><a href=#4-điều-kiện-dừng>4. Điều kiện dừng</a></li><li><a href=#5-các-biến-thể>5. Các biến thể</a><ul><li><a href=#5-1-sgd>5.1. SGD</a></li><li><a href=#5-2-mini-batch-gd>5.2. Mini-batch GD</a></li></ul></li><li><a href=#6-tăng-tốc-gd>6. Tăng tốc GD</a></li><li><a href=#7-kết-luận>7. Kết luận</a></li></ul></nav><h1 id=1-gradient-descent-là-gì>1. Gradient Descent là gì</h1><p>Như trong bài <a href=https://dominhhai.github.io/vi/2017/10/multi-var-func/#4-gradient-v%C3%A0-%C4%91%E1%BA%A1o-h%C3%A0m-c%C3%B3-h%C6%B0%E1%BB%9Bng>đạo hàm của hàm nhiều biến</a> đã giải thích về gradient và sự biến thiên của hàm số thì hàm số sẽ tăng nhanh nhất theo hướng của gradient (<em>gradient ascent</em>) và giảm nhanh nhất theo hướng ngược của gradient (<em>gradient descent</em>).</p><p>Như vậy một cách trực quan ta có thể nhận xét rằng nếu ta cứ đi ngược hướng đạo hàm mãi thì ta sẽ tới được điểm cực tiểu của hàm số. Nếu bạn cần ví dụ minh họa trực quan thì tôi nghĩ nên xem <a href=https://machinelearningcoban.com/2017/01/12/gradientdescent/ target=_blank _ rel="noopener noreferrer">bài viết này</a> của anh Tiệp. Theo như tôi thấy thì bài viết của anh ấy rất rõ ràng và đẩy đủ, nên tôi sẽ không phí công viết lại ở đây nữa mà sẽ tập trung vào khai triển cho lập trình.</p><p>Giả sử ta cần tìm tham số $\theta\in\mathbb{R}^n$ để tối thiểu hoá hàm lỗi $J(\theta)$. Đầu tiên ta sẽ đặt $\theta$ tại một điểm bất kì nào đó. Sau đó giải thuật <em>GD</em> được thực hiện bằng cách cập nhập dần các tham số $\theta$ ngược với hướng của gradient $\nabla_\theta J(\theta)$ tại điểm hiện tại cho tới khi nó hội tụ về điểm nhỏ nhất. Tại mỗi bước cập nhập, ta sẽ dịch tham số bằng một lượng $\eta\nabla_\theta J(\theta)$ với <strong>độ học</strong> (<em>learning rate</em>) $\eta&gt;0$ thể hiện cho việc dịch chuyển nhiều tới đâu:
$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla_\theta J(\theta^{(k)})$$</p><p>$\theta^{(k)}$ ở đây kí hiệu cho tham số tại bước cập nhập lần $k$ khi thực hiện giải thuật <em>GD</em>.</p><p>Ví dụ, ta có hàm $J(\theta) = \theta_0^2+sin(\theta_1)$. Gradient (ma trận Jacobi) lúc này sẽ là:
$$
\nabla_\theta J(\theta)=
\begin{bmatrix}
\dfrac{\partial}{\partial\theta_0}J\cr
\dfrac{\partial}{\partial\theta_1}J
\end{bmatrix}=
\begin{bmatrix}
2\theta_0\cr
\cos(\theta_1)
\end{bmatrix}
$$</p><p>Tại bước bất kì các tham số được cập nhập như sau:
$$
\begin{cases}
\theta_0=\theta_0-\eta2\theta_0 \cr
\theta_1=\theta_1-\eta\cos(\theta_1)
\end{cases}
$$</p><p>Lưu ý rằng ta phải cập nhập đồng thời tham số sau khi tính đạo hàm chứ không được đồng thời vừa tính đạo hàm vừa cập nhập tham số. Ví dụ với <code>Python</code>:<figure class="highlight python"><figcaption><span>gradient_descent.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br><span class=line>13</span><br><span class=line>14</span><br><span class=line>15</span><br><span class=line>16</span><br><span class=line>17</span><br><span class=line>18</span><br><span class=line>19</span><br><span class=line>20</span><br><span class=line>21</span><br><span class=line>22</span><br><span class=line>23</span><br></pre></td><td class=code><pre class="python code-highlight">&#39;&#39;&#39;
Evaluate Gradient for J = theta_0^2 &#43; sin(theta_1)
&#39;&#39;&#39;
def eval_grad(theta):
  # init gradient
  grad = np.empty_like(theta)
  # eval gradient
  grad[0] = 2 * theta[0]
  grad[1] = np.cos(theta[1])
  return grad

# run 1000 times
NUM_INTERS = 1000
# learning rate
ETA = 0.01

# init theta
theta = np.zeros(2)

# run GD
for (i in range(NUM_INTERS)):
  grad = eval_grad(theta)
  theta -= ETA * grad</pre></td></tr></tbody></table></figure></p><p>Việc chọn $\eta$ có ý nghĩa rất lớn trong phương pháp này vì nó quyết định tới tính sống còn của giải thuật. Nếu $\eta$ quá lớn thì ta không hội tụ được về đích, nhưng nếu $\eta$ quá nhỏ thì ta lại mất rất nhiều thời gian để chạy giải thuật này.</p><p>Ngoài ra nếu để ý thì thấy nếu hàm lỗi $J(\theta)$ mà không lồi thì ta rất dễ bị rơi vào điểm tối thiểu cục bộ (<em>local minimum</em>) thay vì tiến được tới điểm tối thiểu toàn cục (<em>global minimum</em>). Việc chọn $\eta$ lúc này có vai trò rất lớn vì nếu $\eta$ hợp lý thì ta có vể vượt qua được điểm tối ưu cục bộ để tiến tiếp tới điểm tối ưu toán cục.</p><p>Thật khó để đưa ra một lời khuyên nào cho việc chọn $\eta$ vì nó còn phụ thuộc vào dữ liệu đang làm việc ra sao nữa. Nhưng thường thì người ta cứ thử nhiều giá trị và đưa ra đánh giá để chọn lấy một giá trị phù hợp. Thường mình hay bắt đầu với $\eta=0.01$ để làm việc, lý do là gì thì cũng không rõ. hé hé</p><p>Một yếu điểm nữa khi cài đặt giải thuật này là việc tính đạo hàm rất dễ bị nhầm lẫn, ta sẽ cùng xem xét trong phần tính đạo hàm dưới đây.</p><h1 id=2-ứng-dụng-cho-hồi-quy-tuyến-tính>2. Ứng dụng cho hồi quy tuyến tính</h1><p>Với bài toán hồi quy tuyến tính ta có công thức tính của hàm lỗi như sau:
$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\theta^{\intercal}\phi(\mathbf{x}_i)-y_i)^2$$
Lúc này ta có gradient:
$$\nabla_\theta J(\theta)=\frac{1}{m}(\theta^{\intercal}\Phi-y)\Phi$$</p><p>Trong đó $\Phi$ là <a href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/#5-k%E1%BA%BFt-lu%E1%BA%ADn>ma trận mẫu</a>:
$$\Phi=
\begin{bmatrix}
\phi_0(\mathbf{x}_1)&amp;\phi_1(\mathbf{x}_1)&amp;&hellip;&amp;\phi_{n-1}(\mathbf{x}_1)\cr
\phi_0(\mathbf{x}_2)&amp;\phi_1(\mathbf{x}_2)&amp;&hellip;&amp;\phi_{n-1}(\mathbf{x}_2)\cr
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr
\phi_0(\mathbf{x}_m)&amp;\phi_1(\mathbf{x}_m)&amp;&hellip;&amp;\phi_{n-1}(\mathbf{x}_m)
\end{bmatrix}
$$</p><p>Như vậy tại mỗi bước tham số được cập nhập:
$$
\theta=\theta-\eta\frac{1}{m}(\theta^{\intercal}\Phi-y)\Phi
$$</p><p>Code với <code>Python</code>:<figure class="highlight python"><figcaption><span>gradient_descent.py</span><a href=https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex1.ipynb target=_blank rel=external>gradient_descent.py</a></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br></pre></td><td class=code><pre class="python code-highlight">def gradient_descent(X, y, theta, eta, num_inters):
    # number of training examples
    m = y.size

    for i in range(num_inters):
        delta = np.dot(np.dot(X, theta) - y, X) / m
        theta -= eta * delta

    return theta</pre></td></tr></tbody></table></figure></p><p>Mã nguồn mẫu đầy đủ bạn có thể xem <a href=https://github.com/dominhhai/mldl/blob/master/coursera-ml/ex1.ipynb target=_blank _ rel="noopener noreferrer">tại đây</a> nhé.</p><h1 id=3-tính-đạo-hàm>3. Tính đạo hàm</h1><h2 id=3-1-tính-đạo-hàm>3.1. Tính đạo hàm</h2><p>Để tính đạo hàm ta có 2 cách:</p><ul><li><p><strong>Phương pháp số học</strong> (<em>numerical gradient</em>): Phương pháp này sẽ lấy đạo hàm bằng cách tính như định nghĩa đạo hàm. Đạo hàm được định nghĩa là: $f^{\prime}(x)=\lim\limits_{h\rightarrow 0}\dfrac{f(x+h)-f(x)}{h}$. Nên nếu ta chọn $h$ rất bé thì có thể coi như lấy được đạo hàm gần đúng. Thường người ta lấy $h=\text{1e-5}$ để tính toán. Và trên thực tế thì khi sử dụng phương pháp này ta sẽ sử dụng đạo hàm 2 phía để tính, tức là: $f^{\prime}(x)=\dfrac{f(x+h)-f(x-h)}{2h}$. Phương pháp này khi thực hiện sẽ rất chậm nên thường người ta không sử dụng trong thực tế.</p></li><li><p><strong>Phương pháp giải tích</strong> (<em>analytic gradient</em>): Phương pháp này sẽ sử dụng các công thức tính đạo hàm trong giải tích để thực hiện. Ví dụ như đạo hàm của $f(x)=2\sin(x)$ sẽ là $f^{\prime}(x)=2\cos(x)$. Phương pháp này thì nhanh nhưng rất dễ nhầm lẫn nếu công thức tính của ta bị sai.</p></li></ul><h2 id=3-2-kiểm-tra-đạo-hàm>3.2. Kiểm tra đạo hàm</h2><p>Việc sử dụng phương pháp giải thích có thể dễ bị nhầm khi cài đặt nên thường người ta phải sử dụng cả phương pháp số học để kiểm tra xem việc tính đạo hàm là đúng hay sai. Nếu kết quả tính bằng giải tích và số học không chênh nhau nhiều thì ta có thể tự tin khẳng định việc cài đặt của ta là đúng đắn.</p><p>$$|f^{\prime}_{NG}-f^{\prime}_{AG}|&lt;\epsilon$$</p><p>Trong đó $f^{\prime}_{NG}$ là đạo hàm tính theo số học và $f^{\prime}_{AG}$ là đạo hàm tính theo giải thích, còn $\epsilon$ là một số dương rất bé. Thường ta lấy $\epsilon=\text{1e-5}$. Lưu ý là với trường hợp nhiều biến thì ta so sánh đạo hàm riêng của từng biến một nhé.</p><p>Do việc tính đạo hàm theo phương pháp số học rất chậm nên thường ta chỉ kiểm tra đạo hàm tại bước cập nhập tham số đầu tiên mà thôi. Tuy nhiễn vẫn có khả năng rủi ro là tính lúc ấy không vấn đề gì nhưng chưa không đảm bảo được chắc chắn rằng việc thực hiện đạo hàm là đúng đắn. Mình đã từng ăn vụ này 1 lần rồi, nên để cho chắc thì kiểm tra ngẫu nhiên vài bước. Chấp nhận đau thương là bị chậm đi còn hơn là đi cả lò bánh!</p><h1 id=4-điều-kiện-dừng>4. Điều kiện dừng</h1><p>Điều kiện dừng là điều kiệu để dừng việc cập nhập tham số lại. Nếu không có nó thì biết khi nào chương trình mới ngừng hoạt động phải không? Mà một chương trình máy tính chỉ có 2 kết cục duy nhất là dừng lại được hoặc sẽ không bao giờ dừng lại được. Nếu bạn quan tâm thì có thể đọc thêm <a href=https://en.wikipedia.org/wiki/Halting_problem target=_blank _ rel="noopener noreferrer">ở đây</a> nhé.</p><p>Thường thì người ta có các cách để thực hiện việc dùng giải thuật sau đây:</p><ul><li><p><strong>Giới hạn số lần cập nhập</strong>: Ta sẽ giới hạn số lần cập nhập tham số để chương trình dừng lại được. Tuy nhiên cách này có nhược điểm là không biết hàm lỗi của ta hội tụ được về đính hay chưa. Nếu mà đen thì ta bị dừng lại ở một giá trị gần với đích hoặc vừa nhảy qua đích một chút. Nếu tăng thêm 1 vài vòng thì có thể vừa tới đích. Nhưng ta lại không kiểm tra được tới đích hay chưa nên vấn đề này rất dễ xảy ra.</p></li><li><p><strong>Kiểm tra giá trị hàm lỗi</strong>: So sánh giá trị của hàm lỗi sau 2 lần cập nhập liên tiếp hoặc sau một số lần cập nhập không quá lớn, nếu giá trị không khác nhau nhiều thì ta có thể coi là đã phần nào hội tụ được về đích rồi và ta sẽ dừng lại. Phương pháp này có nhược điểm là rất dễ bị dừng lại tại điểm mà đồ thị của hàm lỗi bằng phẳng. Chỗ này cực củ chuối! Ta vẫn không có cách nào để kiểm tra được điểm dừng lại là đích hay chưa. Tôi cho rằng bạn nên đọc thêm về vấn đề này <a href=https://en.wikipedia.org/wiki/Saddle_point target=_blank _ rel="noopener noreferrer">tại đây</a>.</p></li><li><p><strong>Kiểm tra giá trị đạo hàm</strong>: So sánh giá trị của gradient sau 2 lần cập nhập liên tiếp hoặc sau một số lần cập nhập không quá lớn, nếu không khác biệt nhiều thì ta có thể dừng chương trình lại. Thường với <em>BGD</em> ta sẽ kiểm tra sau 2 lần cập nhập liên tiếp, còn <em>SGD</em> và <em>MGD</em> thì kiểm tra sau 1 số lần không quá lớn nào đó. Bạn đừng lo nếu chưa hiểu <em>BGD</em>, <em>SGD</em> và <em>MGD</em> là gì ngay nhé, vì ta sẽ xem ngay phần sau đây. Giải thuật này cũng có yếu điểm là tại nơi đạo hàm không khác nhau nhiều lắm thì chưa chắc là đích vì ta cũng chưa có cách nào kiểm tra được điểm hiện tại là đích hay không.</p></li></ul><p>Ngoài các cách trên, ta còn có 1 phương pháp hơi <em>thông minh</em> một tẹo nữa là sau 2 lần cập nhập liên tiếp hoặc sau 1 số lần cập nhập đủ nhỏ, ta so sánh giá trị của hàm lỗi đối với tập huấn luyện (<em>training set</em>) và tập kiểm chứng (<em>validation set</em>), nếu 2 giá trị này càng ngày càng lệch nhau nhiều thì ta nên dừng lại và xem xét phương pháp học của ta. Vì sao lại thế thì tôi sẽ viết ở bài cân bằng hoá giữa độ lệch và phương sai sau, giờ thì bạn cứ tạm hiểu là nếu 2 giá trị đó ngày càng lệch nhau thì thuật toán học của ta đang bị <em>khớp quá</em> (<em>over-fitting</em>) nên việc tính toán tiếp cũng chẳng có tác dụng gì cả.</p><p>Hoặc có thể kiểm tra giá trị của hàm lỗi với tập kiểm chứng nếu giá trị của tập này không giảm được nhiều thì ta có thể coi rằng nó sẽ không thể giảm được nữa, lúc đó ta nên dừng chương trình lại để mất tốn thời gian.</p><h1 id=5-các-biến-thể>5. Các biến thể</h1><p>Dạng cài đặt GD từ đầu bài tới giờ được gọi là GD thuần (<em>Vanilla GD</em>) hay GD toàn phần (<em>BGD - Batch GD</em>). Đặc điểm của nó là sử dụng toàn bộ tập dữ liệu để tính đạo hàm cập nhập tham số nên có thể dẫn tới tình trạng vượt quá khả năng lưu trữ của máy tính và không thể sử dụng được cho các bài toán cần học tức thì (<em>online training</em>).</p><h2 id=5-1-sgd>5.1. SGD</h2><p>Thay vì sử dụng toàn bộ tập dữ liệu để cập nhập tham số thì ta có thể sử dụng từng dữ liệu một để cập nhập. Phương pháp như vậy được gọi là <strong>GD ngẫu nhiên</strong> (<em>SGD - Stochastic Gradient Descent</em>). Về cơ bản ở mỗi lần cập nhập tham số, ta duyệt toàn bộ các cặp mẫu $(\mathbf{x}^{(i)},y^{(i)})$ và cập nhập tương tự như <em>BGD</em> như sau:</p><p>$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla_\theta J(\theta^{(k)};\mathbf{x}^{(i)},y^{(i)})$$</p><p>Vì sử dụng từng mẫu đơn một nên tốc độ tính toán đạo hàm sẽ nhanh hơn rất nhiều so với <em>BGD</em> nhưng nó phải trả cái giá là tốc độ hội tụ bị giảm đi. Một lưu ý khi cài đặt giải thuật này là mỗi bước cập nhập ta nên xáo trộn dữ liệu rồi mới lấy ra cập nhập. Việc này giúp giảm được sự đi lòng vòng về đích của giải thuật vì ta trao khả năng cập nhập ngẫu nhiên cho nó tức là sẽ có cơ hội nhảy được 1 bước xa hơn khi tính toán.</p><p>Code với <code>Python</code>:</p><figure class="highlight python"><figcaption><span>stochastic_gradient_descent.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre></td><td class=code><pre class="python code-highlight">def gradient_descent(X, y, theta, eta, num_inters):
    # number of training examples
    m = y.size

    for i in range(num_inters):
        np.random.shuffle(X)
        for j in X.shape[0]:
            delta = np.dot(np.dot(X[j,:], theta) - y[j], X[j,:]) / m
            theta -= eta * delta

    return theta</pre></td></tr></tbody></table></figure><h2 id=5-2-mini-batch-gd>5.2. Mini-batch GD</h2><p>Do <em>SGD</em> chạy chậm nên người ta thường sử dụng một phương pháp kết hợp giữa <em>BGD</em> và <em>SGD</em> là sử dụng từng nhóm dữ liệu để cập nhập tham số. Tức là ta sẽ chia dữ liệu ra thành nhiều lô khác nhau và mỗi lần cập nhập dữ liệu, thay vì sử dụng từng mẫu một ta sẽ sử dụng cả lô dữ liệu một. Phương pháp như vậy được gọi là <strong>Mini-batch GD</strong> hay viết tắt là <em>MGD</em>. Như vậy ta thấy rằng nếu dữ liệu ta chỉ có 1 lô thì <em>MGD</em> chính là <em>BGD</em>, nếu mỗi lô chỉ có đúng 1 mẫu thì <em>MGD</em> sẽ là <em>SGD</em>.</p><p>Giả sử lô thứ $i$ được kí hiệu là $(\mathbf{X}^{(i)},\mathbf{y}^{(i)})$, thì công thức cập nhập được viết như sau:</p><p>$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla_\theta J(\theta^{(k)};\mathbf{X}^{(i)},\mathbf{y}^{(i)})$$</p><p>Cũng như <em>SGD</em> ta cũng sẽ xáo trộn dữ liệu trước khi phân lô cập nhập tham số:</p><figure class="highlight python"><figcaption><span>mini_batch_gradient_descent.py</span></figcaption><table><tbody><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br></pre></td><td class=code><pre class="python code-highlight">def gradient_descent(X, y, theta, eta, num_inters):
    # number of training examples
    m = y.size

    for i in range(num_inters):
        np.random.shuffle(X)
        while j &lt; X.shape[0]:
            delta = np.dot(np.dot(X[j:j&#43;k,:], theta) - y[j:j&#43;k], X[j:j&#43;k,:]) / m
            theta -= eta * delta

    return theta</pre></td></tr></tbody></table></figure><h1 id=6-tăng-tốc-gd>6. Tăng tốc GD</h1><p>Để tăng tốc cho GD ta có thể sử dụng một số mẹo sau đây:</p><ul><li><p><strong>Song song hoá</strong>: Ta có thể thực hiện việc tính đạo hàm một cách song song bằng cách phân dữ liệu ra cho nhiều phần và sử dụng GPU hoặc nhiều máy tính kết nối với nhau để thực hiện sau đó hợp kết quả lại với nhau. Ví dụ, ta có 5 máy tính trong đó có 1 máy tính được coi là máy chủ sẽ đảm nhận việc thực hiện giải thuật còn 4 máy tính còn lại sẽ sử dụng làm phân tải tính toán. Khi đó ta chia dữ liệu làm 5 phần và phân cho mỗi máy tính thực hiện một phần dữ liệu sau đó hợp kết quả lại để tính được đạo hàm hợp. Tất nhiên là ta phải trả giá ở độ trễ mạng hay switching context nhưng nó vẫn chưa là gì so với việc tính toán hàng triệu dữ liệu với số chiều dữ liệu cũng lên tới cả triệu như vậy.</p></li><li><p><strong>Chuẩn hoá đầu vào</strong>: Việc chuẩn hoá đầu vào cũng khá quan trọng, nếu dữ liệu của ta đẹp - quanh đều quanh tâm đích thì việc hội tụi về sẽ nhanh hơn ở bất kì điểm nào. Nó như việc ta bóp tròn lại một đồ thị hình eclipse. Nếu là tròn thì tại bất kì điểm nào ta cũng có thể hội tụ về đích như nhau, nhưng nếu hình eclipse thì chịu. Việc chuẩn hoá này ta có thể thực hiện bằng cái hàm $\phi(\mathbf{x})$ chẳng hạn. Tôi sẽ đi vào phần này ở các bài viết sau nên tới đây bạn cứ tạm hiểu như vậy đã ha.</p></li></ul><h1 id=7-kết-luận>7. Kết luận</h1><p>Giải thuật GD - Gradient Descent hay còn gọi là <em>BGD</em> là một phương pháp tối ưu dựa trên gradient của hàm số. Giải thuật này sẽ cập nhập các tham số bằng cách đi ngược với gradient cho tới khi hội tụ. Mỗi lần cập nhập, ta cập nhập tham số bằng một lượng $\eta\nabla_\theta J(\theta)$:
$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla_\theta J(\theta^{(k)})$$</p><p>2 biến thể khác của <em>BGD</em> rất hay được sử dụng là <em>SGD - Stochastic Gradient Descent</em> - sử dụng từng mẫu một để cập nhập:
$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla_\theta J(\theta^{(k)};\mathbf{x}^{(i)},y^{(i)})$$
và <em>MGD - Mini-batch Gradient Descent</em> - sử dụng từng lô dữ liệu một để cập nhập:
$$\theta^{(k+1)}=\theta^{(k)}-\eta\nabla_\theta J(\theta^{(k)};\mathbf{X}^{(i)},\mathbf{y}^{(i)})$$</p><p>Việc kiểm tra điều kiện dừng cho giải thuật này là rất quan trọng và gần như là việc bắt buộc khi thực hiện thuật toán. Tuỳ vào bài toán và dữ liệu bạn còn mà có thể chọn lấy 1 cách hoặc kết hợp nhiều phương pháp khác nhau để kiểm tra điều kiện dừng. Tôi thì hay kết hợp việc giới hạn số vòng và kiểm tra sự biến thiên của đạo hàm. Đôi lúc thêm cả việc kiểm tra sự biến thiên của hàm lỗi với tập kiểm chứng nữa để có thể thoát sớm.</p><p>Ngoài các phương pháp tăng tốc giải thuật đã đề cập ra thì còn nhiều giải thuật cải tiến GD nữa mà tôi sẽ viết trong một bài khác. Còn giờ nếu bạn có góp ý gì xin hãy để lại lời nhắn bên dưới nhé.</p></div></div><div id=post-footer class="post-footer main-content-wrap"><div class=post-footer-tags><span class="text-color-light text-small">THẺ ĐÁNH DẤU</span><br><a class="tag tag--primary tag--small" href=https://dominhhai.github.io/vi/tags/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></div><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--disabled"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/ data-tooltip="[ML] MLE của hồi quy tuyến tính"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-gd/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-gd/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-gd/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div><div id=disqus_thread><noscript>Please enable JavaScript to view the <a href=//disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></div></article><footer id=footer class=main-content-wrap><div id=helpinfo><div id=author><label id=home>Hai's Blog</label><div class=language><label for=language>Other Languages:</label>
<select id=language>
<option title="Tiếng Việt" value=vi selected>Tiếng Việt (vi)</option>
<option title=English value=en-us>English (en-us)</option>
<option title=日本語 value=ja>日本語 (ja)</option></select></div></div><div id=topic><label>Chủ đề</label><ul><li><a href=https://dominhhai.github.io/vi/categories/l%E1%BA%ADp-tr%C3%ACnh/>Lập Trình</a></li><li><a href=https://dominhhai.github.io/vi/categories/h%E1%BB%8Dc-m%C3%A1y/>Học Máy</a></li><li><a href=https://dominhhai.github.io/vi/categories/to%C3%A1n/>Toán</a></li><li><a href=https://dominhhai.github.io/vi/categories/x%C3%A1c-su%E1%BA%A5t/>Xác Suất</a></li><li><a href=https://dominhhai.github.io/vi/categories/s%C3%A1ch/>Sách</a></li></ul></div><div id=contact><label>Liên hệ</label><ul><li><a href=https://github.com/dominhhai/dominhhai.github.io/issues/new target=_blank><i class="fa fa-lg fa-inbox"></i>Gửi tin nhắn</a></li><li id=follow><a href=https://github.com/dominhhai target=_blank><i class="sidebar-button-icon fa fa-lg fa-github"></i></a><a href=https://twitter.com/minhhai3b target=_blank><i class="sidebar-button-icon fa fa-lg fa-twitter"></i></a></li></ul></div></div><div id=contentinfo><span class=copyrights>&copy; 2017 <a href=https://github.com/dominhhai>Do Minh Hai</a>. All Rights Reserved</span></div></footer></div><div id=bottom-bar class=post-bottom-bar data-behavior=5><div class=post-actions-wrap><nav><ul class="post-actions post-action-nav"><li class=post-action><a class="post-action-btn btn btn--disabled"><i class="fa fa-angle-left"></i><span class="hide-xs hide-sm text-small icon-ml">Tiếp</span></a></li><li class=post-action><a class="post-action-btn btn btn--default tooltip--top" href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/ data-tooltip="[ML] MLE của hồi quy tuyến tính"><span class="hide-xs hide-sm text-small icon-mr">Trước</span>
<i class="fa fa-angle-right"></i></a></li></ul></nav><ul class="post-actions post-action-share"><li class="post-action hide-lg hide-md hide-sm"><a class="post-action-btn btn btn--default btn-open-shareoptions" href=#btn-open-shareoptions><i class="fa fa-share-alt"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://www.facebook.com/sharer/sharer.php?u=https://dominhhai.github.io/vi/2017/12/ml-gd/"><i class="fa fa-facebook-official"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://twitter.com/intent/tweet?text=https://dominhhai.github.io/vi/2017/12/ml-gd/"><i class="fa fa-twitter"></i></a></li><li class="post-action hide-xs"><a class="post-action-btn btn btn--default" target=new href="https://plus.google.com/share?url=https://dominhhai.github.io/vi/2017/12/ml-gd/"><i class="fa fa-google-plus"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#disqus_thread><i class="fa fa-comment-o"></i></a></li><li class=post-action><a class="post-action-btn btn btn--default" href=#table-of-contents><i class="fa fa-list"></i></a></li></ul></div></div><div id=share-options-bar class=share-options-bar data-behavior=5><i id=btn-close-shareoptions class="fa fa-close"></i><ul class=share-options><li class=share-option><a class=share-option-btn target=new href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-gd%2F"><i class="fa fa-facebook-official"></i><span>Chia sẻ với Facebook</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-gd%2F"><i class="fa fa-twitter"></i><span>Chia sẻ với Twitter</span></a></li><li class=share-option><a class=share-option-btn target=new href="https://plus.google.com/share?url=https%3A%2F%2Fdominhhai.github.io%2Fvi%2F2017%2F12%2Fml-gd%2F"><i class="fa fa-google-plus"></i><span>Chia sẻ với Google&#43;</span></a></li></ul></div><div id=share-options-mask class=share-options-mask></div></div><div id=about><div id=about-card><div id=about-btn-close><i class="fa fa-remove"></i></div><img id=about-card-picture src="https://www.gravatar.com/avatar/711b36be8e444cd1d60b348077bfd752?s=110" alt="Ảnh đại diện"><h4 id=about-card-name>Do Minh Hai</h4><div id=about-card-bio>Just a developer<br>Enjoy life as a journey</div><div id=about-card-job><i class="fa fa-briefcase"></i><br>Freelancer</div><div id=about-card-location><i class="fa fa-map-marker"></i><br>Japan</div></div></div><div id=algolia-search-modal class=modal-container><div class=modal><div class=modal-header><span class=close-button><i class="fa fa-close"></i></span><a href=https://algolia.com target=_blank rel=noopener class="searchby-algolia text-color-light link-unstyled"><span class="searchby-algolia-text text-color-light text-small">by</span>
<img class=searchby-algolia-logo src=https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg></a>
<i class="search-icon fa fa-search"></i><form id=algolia-search-form><input id=algolia-search-input name=search class="form-control input--large search-input" placeholder="Tìm kiếm"></form></div><div class=modal-body><div class="no-result text-color-light text-center">không tìm thấy kết quả</div><div class=results><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-gd/><h3 class=media-heading>[ML] Tối ưu hàm lỗi với Gradient Descent</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Mặc dù sử dụng công thức chuẩn để tìm tham số là có thể thực hiện được, nhưng với tập dữ liệu lớn nhiều chiều trong thực tế thì với máy tính lại không thể thực hiện được do các ràng buộc của bộ nhớ cũng như khả năng tính toán. Chưa kể với nhiều bài toán việc giải được đạo hàm để tìm ra công thức chuẩn là rất khó khăn. Nên trong thực tế giải thuật thay thế là <strong>Gradient Descent</strong> thường được sử dụng.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression-mle/><h3 class=media-heading>[ML] MLE của hồi quy tuyến tính</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Như bài viết trước đã đề cập tới phương pháp ước lượng tham số bằng công thức chuẩn cho thuật toán hồi quy tuyến tính $\hat\theta=(\Phi^{\intercal}\Phi)^{-1}\Phi^{\intercal}\mathbf{y}$ bằng cách lấy đạo hàm hàm lỗi (<em>mean squared error</em>). Có thể bạn sẽ nghi ngờ về mức độ tin cậy thống kê của phương pháp ước lượng đó, nên bài viết này sẽ phân tích lý thuyết xác suất ước lượng bằng <a href=https://dominhhai.github.io/vi/2017/10/sampling-parameters-estimation/#2-2-mle>MLE (<em>Maximum Likelihood Esitmation</em>)</a> xem sao.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-linear-regression/><h3 class=media-heading>[ML] Hồi quy tuyến tính (Linear Regression)</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Học có giám sát (<em>Supervised Learning</em>) được chia ra làm 2 dạng lớn là <strong>hồi quy</strong> (<em>regression</em>) và <strong>phân loại</strong> (<em>classification</em>) dựa trên tập dữ liệu mẫu - tập huấn luyện (<em>training data</em>). Với bài đầu tiên này ta sẽ bắt đầu bằng bài toán hồi quy mà cụ thể là hồi quy tuyến tính (<em>linear regression</em>).</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-prml/><h3 class=media-heading>Pattern Recognition and Machine Learning</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Được coi là sách giáo khoa cho những người làm học máy, cuốn sách này viết về các giải thuật và lý thuyết xây dựng các giải thuật nhận dạng mẫu và học máy. Tuy nhiên lúc mới đọc thì thấy khá khó nhằn nên tôi đã tìm hiểu độ khó các phần đề biết đường mà đọc.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/12/ml-intro/><h3 class=media-heading>[ML] Học máy là gì?</h3></a><span class=media-meta><span class="media-date text-small">Dec 12, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thời gian gần đây AI nổi lên mạnh mẽ xâm nhập vào rất nhiều lĩnh vực trong cuộc sống như tự động dịch thuật, nhận dạng giọng nói, điều khiển tự động, v.v. Nó giờ được coi là xu hướng công nghệ thế giới và nhiều người cho rằng đó là cuộc cách mạng công nghiệp lần thứ 4.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/seiko-no-yotei/><h3 class=media-heading>Điểm cốt lõi để thành công</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather">Cuốn sách tổng hợp nội dung của một số buổi nói chuyện của ông Inamori Kazuo về kinh doanh, làm việc, nhân sinh. Vẫn phong cách quen thuộc về cách nhìn cuộc sống, cách suy nghĩ, cách hành động như trong các cuốn sách khác mà ông đã viết, nhưng trong cuốn này đặc biệt ở chỗ tổng hợp được nhiều nội dung khá cô động mà vẫn không thiếu sót nội dung.</div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/vbnet-oracle-version/><h3 class=media-heading>[.NET] Sài nhiều phiên bản Oracle khi thực thi</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Thông thường khi ta build ứng dụng thì phiên bản Oracle DB ở môi trường phát triển và môi trường thực thi là giống nhau nên không xảy ra vấn đề gì cả. Nhưng nếu ở môi trường phát triển và thực thi khác nhau thì sao?</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/what-is-http2/><h3 class=media-heading>[Web] HTTP2 là gì?</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Nhân tiện bản <code>Node v9x</code> mới ra cho phép ta có thể sử dụng ngay API thử nghiệm <code>HTTP/2</code> nên cũng tò mò tìm hiểu đôi chút xem kiến trúc, đặc điểm và cách sử dụng thế nào.
Sau 2 năm ra chính thức ra lò, phiên bản tiếp theo của <code>HTTP</code> này dần được nhiều máy chủ Web lẫn trình duyệt hỗ trợ bởi tính vượt trội của nó so với phiên bản <code>HTTP/1.1</code>.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/11/about-git/><h3 class=media-heading>[Git] Mô tả về GIT của Linus Torvalds</h3></a><span class=media-meta><span class="media-date text-small">Nov 11, 2017</span></span><div class="media-content hide-xs font-merryweather"><p>Đây là mô tả về GIT mà chủ nhân của nó - ông Linus Torvalds đã viết khi công khai mã nguồn. Cụ thể bài này được copy lại từ <a href=https://github.com/git/git/tree/e83c5163316f89bfbde7d9ab23ca2e25604af290 target=_blank _ rel="noopener noreferrer">Github</a>.</p></div></div><div style=clear:both></div><hr></div><div class=media><div class=media-body><a class=link-unstyled href=https://dominhhai.github.io/vi/2017/10/change-filename-bat/><h3 class=media-heading>[Windows] Đổi tên file với .bat file</h3></a><span class=media-meta><span class="media-date text-small">Oct 10, 2017</span></span><div class="media-content hide-xs font-merryweather">Gần đây Gmail không cho phép gửi các file có đuôi là mã nguồn ngôn ngữ lập trình như .js, .vb chẳng hạn. Ngay cả việc đổi đuôi của các file nén cũng không có hiệu quả như trước, nên buộc phải tìm cách đổi toàn bộ đuôi 1 phát.
Bài viết này sẽ nói về cách thay đổi toàn bộ đuôi file bằng .bat file của Windows, tuy nhiên hoàn toàn có thể sử dụng để làm những chuyện khác với các file này như đổi tên chẳng hạn.</div></div><div style=clear:both></div><hr></div></div></div><div class=modal-footer><p class="results-count text-medium" data-message-zero="không tìm thấy kết quả" data-message-one="tìm thấy 1 kết quả" data-message-other="tìm thấy {n} kết quả">55 posts found</p></div></div></div><div id=cover style=background-image:url(https://dominhhai.github.io/images/cover-v1.2.0.jpg)></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js integrity="sha256-IFHWFEbU2/+wNycDECKgjIRSirRNIDp2acEB5fvdVRU=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js integrity="sha256-+mpyNVJsNt4rVXCw0F+pAOiB3YxmHgrbJsx4ecPuUaI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.js integrity="sha256-vMxgR/7FtLovVA+IPrR7+xTgIgARH7y9VZQnmmi0HDI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.js integrity="sha256-N0qFUh7/9vLvia87dDndewmsgsyYoNkdA212tPc+2NI=" crossorigin=anonymous></script><script src=https://dominhhai.github.io/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js></script><script crossorigin=anonymous integrity=sha384-GR8SEkOO1rBN/jnOcQDFcFmwXAevSLx7/Io9Ps1rkxWp983ZIuUGfxivlF/5f5eJ src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/katex.min.js></script><script crossorigin=anonymous integrity=sha384-cXpztMJlr2xFXyDSIfRWYSMVCXZ9HeGXvzyKTYrn03rsMAlOtIQVzjty5ULbaP8L src=https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha1/contrib/auto-render.min.js></script><script src=https://dominhhai.github.io/js/main.js></script><script>$(document).ready(function(){hljs.configure({classPrefix:'',useBR:false});$('pre.code-highlight, pre > code').each(function(i,block){if(!$(this).hasClass('codeblock')){$(this).addClass('codeblock');}
hljs.highlightBlock(block);});});</script><script>var disqus_config=function(){this.page.url='https:\/\/dominhhai.github.io\/vi\/2017\/12\/ml-gd\/';this.page.identifier='\/vi\/2017\/12\/ml-gd\/'};(function(){if(window.location.hostname=="localhost"){return;}
var d=document,s=d.createElement('script');var disqus_shortname='tranquilpeak';s.src='//'+disqus_shortname+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><script>if(typeof fnMain==='function'){fnMain();}</script></body></html>