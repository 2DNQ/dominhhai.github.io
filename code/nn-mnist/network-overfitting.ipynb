{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Network vs Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting nn-overfitting.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile nn-overfitting.py\n",
    "\"\"\"\n",
    "NN class\n",
    "~~~~~~~~~~~~~~~~\n",
    "Neural Network implement class.\n",
    "This NN use sigmoid as activation functions with cross-entropy cost function.\n",
    "\n",
    "Usage:\n",
    "1. Init NN\n",
    "nn = NN(layers)\n",
    "\n",
    "2. Train\n",
    "nn.train((x, y), epochs, mini_batch_size, eta)\n",
    "\n",
    "3. Predict\n",
    "y = nn.predict(x)\n",
    "\n",
    "4. Test / Evaluation\n",
    "correct_num = nn.evaluate(test_data)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Init NN with ``layers`` size.\n",
    "        ``layers`` is array of layer sizes.\n",
    "        E.x: [3, 4, 5, 2] will create a NN of 4 layers.\n",
    "        In which,\n",
    "          input layer containts 3 nodes,\n",
    "         hidden layer 1 contains 4 nodes, \n",
    "         hidden layer 2 contains 5 nodes, \n",
    "         and, output layer contains 2 nodes\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.L = len(layers)\n",
    "        # ``w `` is a list (L-1) dim np.ndarray of matrix W for each layers\n",
    "        # ``w[0]` is layer 2 (hidden layer 1), ..., ``w[L-2]`` is output layer \n",
    "        # Each row hold weights for inputs (from before layer) of correspoding node (on current layer)\n",
    "        # The first column is bias for correspoding node\n",
    "        self.w = [np.random.randn(l2, l1 + 1)/np.sqrt(l1) for l2, l1 in zip(layers[1:], layers[:-1])]\n",
    "        \n",
    "    def train(self, train_data, epochs, mini_batch_size, eta, lamda=0.0):\n",
    "        \"\"\"\n",
    "        Train NN with train data ``[(x, y)]``.\n",
    "        This use mini-batch SGD method to train the NN.\n",
    "        \"\"\"\n",
    "        # number of training data        \n",
    "        m = len(train_data)\n",
    "        # cost\n",
    "        cost = []\n",
    "        for j in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch {0} begin...'.format(j + 1))\n",
    "            # shuffle data before run\n",
    "            random.shuffle(train_data)\n",
    "            # divide data into mini batchs\n",
    "            for k in range(0, m, mini_batch_size):\n",
    "                mini_batch = train_data[k:k+mini_batch_size]\n",
    "                m_batch = len(mini_batch)\n",
    "                # calc gradient\n",
    "                w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "                for x, y in mini_batch:\n",
    "                    grad = self.backprop(x, y)\n",
    "                    w_grad = [W_grad + g for W_grad, g in zip(w_grad, grad)]\n",
    "#                 w_grad = [W_grad / m_batch for W_grad in w_grad]\n",
    "                w_grad = [(W_grad + lamda * np.insert(W[:,1:], 0, 0, axis=1)) / m_batch\n",
    "                                      for W, W_grad in zip(self.w, w_grad)]\n",
    "                \n",
    "                # check grad for first mini_batch in first epoch\n",
    "                #if j == 0  and k == 0 and not self.check_grad(mini_batch, lamda, w_grad):\n",
    "                #    print('backprop fail!')\n",
    "                #    return False\n",
    "                \n",
    "                # update w\n",
    "                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]\n",
    "            \n",
    "            # calc cost\n",
    "            cost_j = self.cost(train_data, lamda)\n",
    "            cost.append(cost_j)\n",
    "            print('Epoch {0} done: {1}, cost: {2}'.format(j + 1, time.time() - start_time, cost_j))\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict label of single input data ``x``\n",
    "        \"\"\"\n",
    "        _, a = self.feedforward(x)\n",
    "        return np.argmax(a[-1])\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluate NN with test data.\n",
    "        This will return the number of correct result\n",
    "        \"\"\"\n",
    "        results = [(self.predict(x), y) for (x, y) in test_data]\n",
    "        return sum(int(_y == y) for (_y, y) in results)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Feedforward through network for calc ``z``,`` a``.\n",
    "        ``z`` is list of (L-1) vec-tor, ``z[0]`` for layer 2, and so on.\n",
    "        ``a`` is list of (L) vec-tor, ``a[0]`` for layer 1, and so on.\n",
    "        \"\"\"\n",
    "        z = []\n",
    "        a = [self.add_bias(x)]\n",
    "        for l in range(1, self.L):\n",
    "            z_l = np.dot(self.w[l-1], a[l-1])\n",
    "            a_l = self.sigmoid(z_l)\n",
    "            if l < self.L - 1:\n",
    "                a_l = self.add_bias(a_l)\n",
    "            \n",
    "            z.append(z_l)\n",
    "            a.append(a_l)\n",
    "            \n",
    "        return (z, a)\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation to calc derivatives\n",
    "        \"\"\"\n",
    "        w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "        # feedforward\n",
    "        z, a = self.feedforward(x)\n",
    "        # backward\n",
    "        dz = a[-1] - y\n",
    "        for _l in range(1, self.L):\n",
    "            l = -_l # layer index\n",
    "            if l < -1:\n",
    "                da = self.sigmoid_grad(z[l])\n",
    "                # do not calc for w_0 (da_0 / dz = 0 because of a_0 = 1 for all z)\n",
    "                dz = np.dot(self.w[l+1][:, 1:].transpose(), dz) * da\n",
    "            # gradient    \n",
    "            w_grad[l] = np.dot(dz, a[l-1].transpose())\n",
    "        \n",
    "        return w_grad\n",
    "    \n",
    "    def add_bias(self, a):\n",
    "        \"\"\"\n",
    "        add a_0 = 1 as input for bias w_0\n",
    "        \"\"\"\n",
    "        return np.insert(a, 0, 1, axis=0)\n",
    "    \n",
    "    def check_grad(self, data, lamda, grad, epsilon=1e-4, threshold=1e-6):\n",
    "        \"\"\"\n",
    "        Check gradient with:\n",
    "        * Epsilon      : 1e-4\n",
    "        * Threshold : 1e-6\n",
    "        \"\"\"\n",
    "        for l in range(self.L - 1):\n",
    "            n_row, n_col = self.w[l].shape\n",
    "            for i in range(n_row):\n",
    "                for j in range(n_col):\n",
    "                    w_l_ij = self.w[l][i][j]\n",
    "                    # left\n",
    "                    self.w[l][i][j] = w_l_ij - epsilon\n",
    "                    l_cost = self.cost(data, lamda)\n",
    "                    # right\n",
    "                    self.w[l][i][j] = w_l_ij + epsilon\n",
    "                    r_cost = self.cost(data, lamda)\n",
    "                    # numerical grad\n",
    "                    num_grad = (r_cost - l_cost) / (2 * epsilon)\n",
    "                    \n",
    "                    # diff\n",
    "                    diff = abs(grad[l][i][j] - num_grad)\n",
    "                    \n",
    "                    # reset w\n",
    "                    self.w[l][i][j] = w_l_ij\n",
    "                    \n",
    "                    if diff > threshold:\n",
    "                        print('Check Grad Error at (l: {0}, col: {1}, row: {2}), | num_grad: {3} vs backprop grad: {4} | : {5}'\n",
    "                              .format(l, i, j, num_grad, grad[l][i][j], diff))\n",
    "                        return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def cost(self, data, lamda):\n",
    "        \"\"\"\n",
    "        Return cross-entropy cost of NN on test data\n",
    "        \"\"\"\n",
    "        m = len(data)\n",
    "        j = 0\n",
    "        for x, y in data:\n",
    "            _, a = self.feedforward(x)\n",
    "            a_L = a[-1]\n",
    "            j += np.sum(np.nan_to_num(y*np.log(a_L) + (1-y)*np.log(1-a_L)))\n",
    "        \n",
    "        j -= 0.5 * lamda * sum(np.linalg.norm(W[:,1:])**2 for W in self.w)\n",
    "        return -j / m\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function use as activation function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_grad(self, z):\n",
    "        \"\"\"\n",
    "        Result derivative of sigmoid function\n",
    "        \"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data: 50000 / validation_data: 10000 / test_data: 10000\n",
      "Epoch 1 begin...\n",
      "Epoch 1 done: 47.5498650074, cost: 0.467022533249\n",
      "Epoch 2 begin...\n",
      "Epoch 2 done: 45.1990818977, cost: 0.368611612898\n",
      "Epoch 3 begin...\n",
      "Epoch 3 done: 47.0702090263, cost: 0.268267454269\n",
      "Epoch 4 begin...\n",
      "Epoch 4 done: 45.2611830235, cost: 0.224818696546\n",
      "Epoch 5 begin...\n",
      "Epoch 5 done: 46.3051109314, cost: 0.206488320479\n",
      "Epoch 6 begin...\n",
      "Epoch 6 done: 43.4128978252, cost: 0.201353975306\n",
      "Epoch 7 begin...\n",
      "Epoch 7 done: 44.047991991, cost: 0.175973512226\n",
      "Epoch 8 begin...\n",
      "Epoch 8 done: 45.7246460915, cost: 0.145440099331\n",
      "Epoch 9 begin...\n",
      "Epoch 9 done: 45.7401850224, cost: 0.119523335954\n",
      "Epoch 10 begin...\n",
      "Epoch 10 done: 48.9777288437, cost: 0.116238369719\n",
      "Epoch 11 begin...\n",
      "Epoch 11 done: 43.8798468113, cost: 0.110199481435\n",
      "Epoch 12 begin...\n",
      "Epoch 12 done: 45.5568561554, cost: 0.102399300606\n",
      "Epoch 13 begin...\n",
      "Epoch 13 done: 45.6248772144, cost: 0.0837667632139\n",
      "Epoch 14 begin...\n",
      "Epoch 14 done: 43.5404942036, cost: 0.0781395830868\n",
      "Epoch 15 begin...\n",
      "Epoch 15 done: 43.1948680878, cost: 0.0863939903911\n",
      "Epoch 16 begin...\n",
      "Epoch 16 done: 46.2410099506, cost: 0.0675141955271\n",
      "Epoch 17 begin...\n",
      "Epoch 17 done: 45.7984790802, cost: 0.0602712646755\n",
      "Epoch 18 begin...\n",
      "Epoch 18 done: 50.1346218586, cost: 0.0498740388713\n",
      "Epoch 19 begin...\n",
      "Epoch 19 done: 50.0769510269, cost: 0.0550697474111\n",
      "Epoch 20 begin...\n",
      "Epoch 20 done: 44.772990942, cost: 0.0477432657745\n",
      "Epoch 21 begin...\n",
      "Epoch 21 done: 46.8327629566, cost: 0.0385031335552\n",
      "Epoch 22 begin...\n",
      "Epoch 22 done: 43.5327618122, cost: 0.0354239136657\n",
      "Epoch 23 begin...\n",
      "Epoch 23 done: 44.7651059628, cost: 0.0378507579549\n",
      "Epoch 24 begin...\n",
      "Epoch 24 done: 43.2776520252, cost: 0.0331383517024\n",
      "Epoch 25 begin...\n",
      "Epoch 25 done: 42.4510068893, cost: 0.0290823209161\n",
      "Epoch 26 begin...\n",
      "Epoch 26 done: 42.6402430534, cost: 0.0254592425017\n",
      "Epoch 27 begin...\n",
      "Epoch 27 done: 42.7322938442, cost: 0.021243308048\n",
      "Epoch 28 begin...\n",
      "Epoch 28 done: 42.6635229588, cost: 0.0220723289342\n",
      "Epoch 29 begin...\n",
      "Epoch 29 done: 42.8865118027, cost: 0.0193777866398\n",
      "Epoch 30 begin...\n",
      "Epoch 30 done: 42.3517401218, cost: 0.0159969919486\n",
      "Evaluation: 9672 / 10000 = 96.72%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import data_loader\n",
    "#from nn_overfitting import NN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data\n",
    "training_data, validation_data, test_data = data_loader.load()\n",
    "print('training_data: {0} / validation_data: {1} / test_data: {2}'.format(len(training_data), len(validation_data), len(test_data)))\n",
    "\n",
    "# run NN\n",
    "nn = NN([784, 100, 10])\n",
    "cost = nn.train(training_data, 30, 10, 0.5, 4.0)\n",
    "\n",
    "correct = nn.evaluate(test_data)\n",
    "total = len(test_data) \n",
    "print('Evaluation: {0} / {1} = {2}%'.format(correct, total, 100 * correct/total))\n",
    "\n",
    "plt.plot(np.arange(1, 31), cost)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
