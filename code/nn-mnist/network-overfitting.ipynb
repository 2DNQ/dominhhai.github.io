{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Network vs Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile nn-overfitting.py\n",
    "\"\"\"\n",
    "NN class\n",
    "~~~~~~~~~~~~~~~~\n",
    "Neural Network implement class.\n",
    "This NN use sigmoid as activation functions with cross-entropy cost function.\n",
    "\n",
    "Usage:\n",
    "1. Init NN\n",
    "nn = NN(layers)\n",
    "\n",
    "2. Train\n",
    "nn.train((x, y), epochs, mini_batch_size, eta)\n",
    "\n",
    "3. Predict\n",
    "y = nn.predict(x)\n",
    "\n",
    "4. Test / Evaluation\n",
    "correct_num = nn.evaluate(test_data)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"\n",
    "        Init NN with ``layers`` size.\n",
    "        ``layers`` is array of layer sizes.\n",
    "        E.x: [3, 4, 5, 2] will create a NN of 4 layers.\n",
    "        In which,\n",
    "          input layer containts 3 nodes,\n",
    "         hidden layer 1 contains 4 nodes, \n",
    "         hidden layer 2 contains 5 nodes, \n",
    "         and, output layer contains 2 nodes\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.L = len(layers)\n",
    "        # ``w `` is a list (L-1) dim np.ndarray of matrix W for each layers\n",
    "        # ``w[0]` is layer 2 (hidden layer 1), ..., ``w[L-2]`` is output layer \n",
    "        # Each row hold weights for inputs (from before layer) of correspoding node (on current layer)\n",
    "        # The first column is bias for correspoding node\n",
    "        self.w = [np.random.randn(l2, l1 + 1)/np.sqrt(l1) for l2, l1 in zip(layers[1:], layers[:-1])]\n",
    "        \n",
    "    def train(self, train_data, epochs, mini_batch_size, eta, lamda=0.0):\n",
    "        \"\"\"\n",
    "        Train NN with train data ``[(x, y)]``.\n",
    "        This use mini-batch SGD method to train the NN.\n",
    "        \"\"\"\n",
    "        # number of training data        \n",
    "        m = len(train_data)\n",
    "        # cost\n",
    "        cost = []\n",
    "        for j in range(epochs):\n",
    "            start_time = time.time()\n",
    "            print('Epoch {0} begin...'.format(j + 1))\n",
    "            # shuffle data before run\n",
    "            random.shuffle(train_data)\n",
    "            # divide data into mini batchs\n",
    "            for k in range(0, m, mini_batch_size):\n",
    "                mini_batch = train_data[k:k+mini_batch_size]\n",
    "                m_batch = len(mini_batch)\n",
    "                # calc gradient\n",
    "                w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "                for x, y in mini_batch:\n",
    "                    grad = self.backprop(x, y)\n",
    "                    w_grad = [W_grad + g for W_grad, g in zip(w_grad, grad)]\n",
    "#                 w_grad = [W_grad / m_batch for W_grad in w_grad]\n",
    "                w_grad = [(W_grad + lamda * np.insert(W[:,1:], 0, 0, axis=1)) / m_batch\n",
    "                                      for W, W_grad in zip(self.w, w_grad)]\n",
    "                \n",
    "                # check grad for first mini_batch in first epoch\n",
    "                #if j == 0  and k == 0 and not self.check_grad(mini_batch, lamda, w_grad):\n",
    "                #    print('backprop fail!')\n",
    "                #    return False\n",
    "                \n",
    "                # update w\n",
    "                self.w = [W - eta * W_grad for W, W_grad in zip(self.w, w_grad)]\n",
    "            \n",
    "            # calc cost\n",
    "            cost_j = self.cost(train_data, lamda)\n",
    "            cost.append(cost_j)\n",
    "            print('Epoch {0} done: {1}, cost: {2}'.format(j + 1, time.time() - start_time, cost_j))\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict label of single input data ``x``\n",
    "        \"\"\"\n",
    "        _, a = self.feedforward(x)\n",
    "        return np.argmax(a[-1])\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Evaluate NN with test data.\n",
    "        This will return the number of correct result\n",
    "        \"\"\"\n",
    "        results = [(self.predict(x), y) for (x, y) in test_data]\n",
    "        return sum(int(_y == y) for (_y, y) in results)\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Feedforward through network for calc ``z``,`` a``.\n",
    "        ``z`` is list of (L-1) vec-tor, ``z[0]`` for layer 2, and so on.\n",
    "        ``a`` is list of (L) vec-tor, ``a[0]`` for layer 1, and so on.\n",
    "        \"\"\"\n",
    "        z = []\n",
    "        a = [self.add_bias(x)]\n",
    "        for l in range(1, self.L):\n",
    "            z_l = np.dot(self.w[l-1], a[l-1])\n",
    "            a_l = self.sigmoid(z_l)\n",
    "            if l < self.L - 1:\n",
    "                a_l = self.add_bias(a_l)\n",
    "            \n",
    "            z.append(z_l)\n",
    "            a.append(a_l)\n",
    "            \n",
    "        return (z, a)\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Backpropagation to calc derivatives\n",
    "        \"\"\"\n",
    "        w_grad = [np.zeros(W.shape) for W in self.w]\n",
    "        # feedforward\n",
    "        z, a = self.feedforward(x)\n",
    "        # backward\n",
    "        dz = a[-1] - y\n",
    "        for _l in range(1, self.L):\n",
    "            l = -_l # layer index\n",
    "            if l < -1:\n",
    "                da = self.sigmoid_grad(z[l])\n",
    "                # do not calc for w_0 (da_0 / dz = 0 because of a_0 = 1 for all z)\n",
    "                dz = np.dot(self.w[l+1][:, 1:].transpose(), dz) * da\n",
    "            # gradient    \n",
    "            w_grad[l] = np.dot(dz, a[l-1].transpose())\n",
    "        \n",
    "        return w_grad\n",
    "    \n",
    "    def add_bias(self, a):\n",
    "        \"\"\"\n",
    "        add a_0 = 1 as input for bias w_0\n",
    "        \"\"\"\n",
    "        return np.insert(a, 0, 1, axis=0)\n",
    "    \n",
    "    def check_grad(self, data, lamda, grad, epsilon=1e-4, threshold=1e-6):\n",
    "        \"\"\"\n",
    "        Check gradient with:\n",
    "        * Epsilon      : 1e-4\n",
    "        * Threshold : 1e-6\n",
    "        \"\"\"\n",
    "        for l in range(self.L - 1):\n",
    "            n_row, n_col = self.w[l].shape\n",
    "            for i in range(n_row):\n",
    "                for j in range(n_col):\n",
    "                    w_l_ij = self.w[l][i][j]\n",
    "                    # left\n",
    "                    self.w[l][i][j] = w_l_ij - epsilon\n",
    "                    l_cost = self.cost(data, lamda)\n",
    "                    # right\n",
    "                    self.w[l][i][j] = w_l_ij + epsilon\n",
    "                    r_cost = self.cost(data, lamda)\n",
    "                    # numerical grad\n",
    "                    num_grad = (r_cost - l_cost) / (2 * epsilon)\n",
    "                    \n",
    "                    # diff\n",
    "                    diff = abs(grad[l][i][j] - num_grad)\n",
    "                    \n",
    "                    # reset w\n",
    "                    self.w[l][i][j] = w_l_ij\n",
    "                    \n",
    "                    if diff > threshold:\n",
    "                        print('Check Grad Error at (l: {0}, col: {1}, row: {2}), | num_grad: {3} vs backprop grad: {4} | : {5}'\n",
    "                              .format(l, i, j, num_grad, grad[l][i][j], diff))\n",
    "                        return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def cost(self, data, lamda):\n",
    "        \"\"\"\n",
    "        Return cross-entropy cost of NN on test data\n",
    "        \"\"\"\n",
    "        m = len(data)\n",
    "        j = 0\n",
    "        for x, y in data:\n",
    "            _, a = self.feedforward(x)\n",
    "            a_L = a[-1]\n",
    "            j += np.sum(np.nan_to_num(y*np.log(a_L) + (1-y)*np.log(1-a_L)))\n",
    "        \n",
    "        j += 0.5 * lamda * sum(np.linalg.norm(W[:,1:])**2 for W in self.w)\n",
    "        return -j / m\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Sigmoid function use as activation function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_grad(self, z):\n",
    "        \"\"\"\n",
    "        Result derivative of sigmoid function\n",
    "        \"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data: 50000 / validation_data: 10000 / test_data: 10000\n",
      "Epoch 1 begin...\n",
      "Epoch 1 done: 63.6525790691, cost: 0.445968056891\n",
      "Epoch 2 begin...\n",
      "Epoch 2 done: 65.1051170826, cost: 0.315574092985\n",
      "Epoch 3 begin...\n",
      "Epoch 3 done: 63.8777930737, cost: 0.231688928033\n",
      "Epoch 4 begin...\n",
      "Epoch 4 done: 61.7526550293, cost: 0.205588335579\n",
      "Epoch 5 begin...\n",
      "Epoch 5 done: 62.3045260906, cost: 0.190258658623\n",
      "Epoch 6 begin...\n",
      "Epoch 6 done: 61.3708639145, cost: 0.159495285172\n",
      "Epoch 7 begin...\n",
      "Epoch 7 done: 61.7806639671, cost: 0.13543813108\n",
      "Epoch 8 begin...\n",
      "Epoch 8 done: 61.7387399673, cost: 0.121164132367\n",
      "Epoch 9 begin...\n",
      "Epoch 9 done: 62.18085289, cost: 0.10602648618\n",
      "Epoch 10 begin...\n",
      "Epoch 10 done: 63.849832058, cost: 0.105662028332\n",
      "Epoch 11 begin...\n",
      "Epoch 11 done: 61.8785939217, cost: 0.0996589288568\n",
      "Epoch 12 begin...\n",
      "Epoch 12 done: 63.0874350071, cost: 0.0911859841313\n",
      "Epoch 13 begin...\n",
      "Epoch 13 done: 62.884912014, cost: 0.0735965293834\n",
      "Epoch 14 begin...\n",
      "Epoch 14 done: 62.2494580746, cost: 0.0632056580741\n",
      "Epoch 15 begin...\n",
      "Epoch 15 done: 61.8945500851, cost: 0.0501375062386\n",
      "Epoch 16 begin...\n",
      "Epoch 16 done: 62.4274539948, cost: 0.0408801530424\n",
      "Epoch 17 begin...\n",
      "Epoch 17 done: 63.7059779167, cost: 0.0337176415662\n",
      "Epoch 18 begin...\n",
      "Epoch 18 done: 63.1142938137, cost: 0.0310602621855\n",
      "Epoch 19 begin...\n",
      "Epoch 19 done: 62.6300990582, cost: 0.0286616795638\n",
      "Epoch 20 begin...\n",
      "Epoch 20 done: 63.0004570484, cost: 0.0240508105501\n",
      "Epoch 21 begin...\n",
      "Epoch 21 done: 62.748611927, cost: 0.0167650077192\n",
      "Epoch 22 begin...\n",
      "Epoch 22 done: 69.7181839943, cost: 0.0111847217426\n",
      "Epoch 23 begin...\n",
      "Epoch 23 done: 66.8161921501, cost: 0.0094814478958\n",
      "Epoch 24 begin...\n",
      "Epoch 24 done: 62.970375061, cost: 0.00731310538034\n",
      "Epoch 25 begin...\n",
      "Epoch 25 done: 64.2326610088, cost: 0.00598302045766\n",
      "Epoch 26 begin...\n",
      "Epoch 26 done: 63.1619169712, cost: 0.00583582076462\n",
      "Epoch 27 begin...\n"
     ]
    }
   ],
   "source": [
    "import data_loader\n",
    "#from nn_overfitting import NN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data\n",
    "training_data, validation_data, test_data = data_loader.load()\n",
    "print('training_data: {0} / validation_data: {1} / test_data: {2}'.format(len(training_data), len(validation_data), len(test_data)))\n",
    "\n",
    "# run NN\n",
    "nn = NN([784, 100, 100, 10])\n",
    "cost = nn.train(training_data, 30, 10, 0.5, 0.0)\n",
    "\n",
    "correct = nn.evaluate(test_data)\n",
    "total = len(test_data) \n",
    "print('Evaluation: {0} / {1} = {2}%'.format(correct, total, 100 * correct/total))\n",
    "\n",
    "plt.plot(np.arange(1, 31), cost)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('cost')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
